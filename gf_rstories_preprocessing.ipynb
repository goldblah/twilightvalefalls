{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-04-19T16:42:00.134186Z",
          "start_time": "2019-04-19T16:40:42.051329Z"
        },
        "pycharm": {
          "is_executing": false
        }
      },
      "outputs": [],
      "source": "import nltk\nimport textblob\nfrom textblob.tokenizers import WordTokenizer\nimport pandas as pd\nimport re\nimport os\nimport sentiment_analysis_functions as saf\nfrom pycontractions import Contractions\nfrom spellchecker import SpellChecker\ndir \u003d \"/volumes/Hayley\u0027s Drive/PycharmProjects/twilightvalefalls/\"\n%run named_entities_all/NER_Functions.py\ncont \u003d Contractions(api_key\u003d\"glove-twitter-25\")\ncont.load_models()"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-04-19T16:42:09.286956Z",
          "start_time": "2019-04-19T16:42:09.276152Z"
        },
        "pycharm": {
          "is_executing": false,
          "metadata": false,
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": "def contract_handle(st):\n    #print(\u0027Doing text %d...\u0027 %texts.index(st))\n    t \u003d list(cont.expand_texts([st.replace(\"â€™\",\"\u0027\")]))[0]\n    tags \u003d nltk.pos_tag(nltk.word_tokenize(str(t)))\n    temp \u003d []\n    print(\u0027%d tags created...\u0027 %len(tags))\n    for tag in tags:\n        temp.append(tag[0])\n            \n    return \u0027 \u0027.join(temp)"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "pycharm": {
          "is_executing": false,
          "metadata": false,
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": "rstories \u003d pd.read_csv(dir + \u0027rstories/rs_df.csv\u0027, sep\u003d\u0027|\u0027, index_col\u003d0)\n\nwith open(dir + \u0027named_entities_all/rstories_ner.txt\u0027, \u0027r\u0027, encoding\u003d\"utf-8\") as myfile:\n        rstories_ner \u003d myfile.read().split(\u0027\\n\u0027)\n\nrstories_ner_split \u003d list()\nfor ner in rstories_ner:\n    rstories_ner_split.append(ner.split(\u0027,\u0027))\n    \nrs_all_names \u003d list()\nfor ner in rstories_ner_split:\n    rs_all_names.append(ner[0])\n\nrs_spelled_words \u003d set(nltk.word_tokenize(\u0027 \u0027.join(rs_all_names).lower()))"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "outputs": [
        {
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m~/anaconda3/envs/nlp2/lib/python3.6/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2656\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-\u003e 2657\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2658\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: \u0027handled_text\u0027",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m\u003cipython-input-6-c99955b79e1d\u003e\u001b[0m in \u001b[0;36m\u003cmodule\u003e\u001b[0;34m\u001b[0m\n\u001b[0;32m----\u003e 1\u001b[0;31m \u001b[0mhandled_stories\u001b[0m \u001b[0;34m\u003d\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrstories\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\u0027handled_text\u0027\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mrstories_sa\u001b[0m \u001b[0;34m\u003d\u001b[0m \u001b[0msaf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msentiment_analysis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdir\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\u0027named_entities_all/rstories_ner.txt\u0027\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtest_bigrams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_sentences\u001b[0m \u001b[0;34m\u003d\u001b[0m \u001b[0mrstories_sa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstory_analysis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandled_stories\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/anaconda3/envs/nlp2/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2925\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m\u003e\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2926\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-\u003e 2927\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m\u003d\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2928\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2929\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m\u003d\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/anaconda3/envs/nlp2/lib/python3.6/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2657\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2658\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-\u003e 2659\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_cast_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2660\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[0;34m\u003d\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m\u003d\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[0;34m\u003d\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2661\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m\u003e\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m \u001b[0;34m\u003e\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: \u0027handled_text\u0027"
          ],
          "ename": "KeyError",
          "evalue": "\u0027handled_text\u0027",
          "output_type": "error"
        }
      ],
      "source": "handled_stories \u003d list(rstories[\u0027handled_text\u0027])\nrstories_sa \u003d saf.sentiment_analysis(dir + \u0027named_entities_all/rstories_ner.txt\u0027)\ntest_bigrams, test_sentences \u003d rstories_sa.story_analysis(handled_stories[0])",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n",
          "is_executing": false
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "pycharm": {
          "is_executing": false,
          "metadata": false,
          "name": "#%%\n"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "text": [
            "1607 tags created...\n",
            "739 tags created...\n",
            "605 tags created...\n",
            "346 tags created...\n",
            "352 tags created...\n",
            "749 tags created...\n",
            "1465 tags created...\n"
          ],
          "output_type": "stream"
        }
      ],
      "source": "############### DONT RUN ME #########################\nrs \u003d list(rstories[\u0027text\u0027])\nhandled_stories \u003d []\nfor story in rs:\n    handled_stories.append(contract_handle(story))"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "outputs": [],
      "source": "############### DONT RUN ME #########################\nrstories[\u0027handled_text\u0027] \u003d handled_stories\nrstories.to_csv(dir + \u0027rstories/rs_df.csv\u0027, sep\u003d\u0027|\u0027)",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n",
          "is_executing": false
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": "rs_spell \u003d SpellChecker()\nrs_spell.word_frequency.load_words(rs_all_names)\nrs_spell.word_frequency.load_words(rs_spelled_words)\nhandled_stories \u003d list(rstories[\u0027handled_text\u0027])\nfor story in handled_stories:\n    words \u003d nltk.word_tokenize(story)\n    for word in words:\n        if str(word) !\u003d rs_spell.correction(str(word)):\n            print(word, rs_spell.correction(str(word)))",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n"
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-04-19T16:42:18.804955Z",
          "start_time": "2019-04-19T16:42:18.578789Z"
        },
        "pycharm": {
          "is_executing": false,
          "metadata": false,
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": "gravityfalls \u003d pd.read_csv(dir + \u0027gravityfalls/gf_eps.csv\u0027, sep\u003d\u0027|\u0027, index_col\u003d0)\n\nwith open(dir + \u0027named_entities_all/gravity_falls_ner.txt\u0027, \u0027r\u0027, encoding\u003d\"utf-8\") as myfile:\n        gravityfalls_ner \u003d myfile.read().split(\u0027\\n\u0027)\n\ngravityfalls_ner_split \u003d list()\nfor ner in gravityfalls_ner:\n    gravityfalls_ner_split.append(ner.split(\u0027,\u0027))\n\ngf_all_names \u003d list()\nfor ner in gravityfalls_ner_split:\n    gf_all_names.append(ner[0])\n\ngf_spelled_words \u003d set(nltk.word_tokenize(\u0027 \u0027.join(gf_all_names).lower()))"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": "############### DONT RUN ME #########################\ngf \u003d list(gravityfalls[\u0027text\u0027])\nhandled_stories \u003d []\nfor story in gf:\n    handled_stories.append(contract_handle(story))",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n"
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": "############### DONT RUN ME #########################\ngravityfalls[\u0027handled_text\u0027] \u003d handled_stories\ngravityfalls.to_csv(dir + \u0027gravityfalls/gf_eps.csv\u0027, sep\u003d\u0027|\u0027)",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n"
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "start_time": "2019-04-19T16:42:25.326Z"
        },
        "pycharm": {
          "is_executing": true,
          "metadata": false,
          "name": "#%%\n"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Beginning contraction handling\n",
            "Completed expansion\n",
            "6159 tags created...\n",
            "Beginning Spell Check\n",
            "Finished Spell Check\n",
            "Beginning contraction handling\n",
            "Completed expansion\n",
            "6619 tags created...\n",
            "Beginning Spell Check\n"
          ]
        }
      ],
      "source": "gf_spell \u003d SpellChecker()\ngf_spell.word_frequency.load_words(gf_spelled_words)\ncounter \u003d 0\nhandled_stories \u003d list(gravityfalls[\u0027handled_text\u0027])\nmispelled \u003d []\nfor story in handled_stories:\n    print(\u0027Beginning Spell Check\u0027)\n    words \u003d nltk.word_tokenize(story)\n    for word in words:\n        if str(word) !\u003d gf_spell.correction(str(word)):\n            counter +\u003d 1\n            mispelled.append(list(zip(str(word),gf_spell.correction(str(word)))))\n    print(\u0027Finished Spell Check\u0027)\nprint(counter)"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": [
            "Beginning Spell Check\n",
            "\n",
            "... p.\n\n\naroo arco\n\n",
            "\n",
            "noc.o.d n.c.o.\n\n"
          ],
          "output_type": "stream"
        }
      ],
      "source": "gf_spell \u003d SpellChecker()\ngf_spell.word_frequency.load_words(gf_spelled_words)\ncounter \u003d 0\nmispelled \u003d []\nhandled_stories \u003d list(gravityfalls[\u0027handled_text\u0027])\nprint(\u0027Beginning Spell Check\u0027)\nwords \u003d nltk.word_tokenize(handled_stories[3])\nfor word in words:\n    if str(word) !\u003d gf_spell.correction(str(word)):\n        print()\n        counter +\u003d 1\n        mispelled.append(list(zip(str(word),gf_spell.correction(str(word)))))\n        print(str(word),gf_spell.correction(str(word)))\n        print()\nprint(\u0027Finished Spell Check\u0027)\nprint(counter)",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n",
          "is_executing": true
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": "\n",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n"
        }
      }
    }
  ],
  "metadata": {
    "hide_input": false,
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}