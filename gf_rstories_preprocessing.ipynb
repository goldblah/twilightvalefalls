{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-04-19T16:42:00.134186Z",
          "start_time": "2019-04-19T16:40:42.051329Z"
        },
        "pycharm": {
          "is_executing": false
        }
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "import textblob\n",
        "from textblob.tokenizers import WordTokenizer\n",
        "import pandas as pd\n",
        "import re\n",
        "import os\n",
        "from pycontractions import Contractions\n",
        "from spellchecker import SpellChecker\n",
        "dir \u003d \"/volumes/Hayley\u0027s Drive/PycharmProjects/twilightvalefalls/\"\n",
        "%run named_entities_all/NER_Functions.py\n",
        "cont \u003d Contractions(api_key\u003d\"glove-twitter-25\")\n",
        "cont.load_models()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-04-19T16:42:09.286956Z",
          "start_time": "2019-04-19T16:42:09.276152Z"
        },
        "pycharm": {
          "is_executing": false,
          "metadata": false,
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": "def contract_handle(st):\n    #print(\u0027Doing text %d...\u0027 %texts.index(st))\n    t \u003d list(cont.expand_texts([st.replace(\"â€™\",\"\u0027\")]))[0]\n    tags \u003d nltk.pos_tag(nltk.word_tokenize(str(t)))\n    temp \u003d []\n    print(\u0027%d tags created...\u0027 %len(tags))\n    for tag in tags:\n        temp.append(tag[0])\n            \n    return \u0027 \u0027.join(temp)"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "pycharm": {
          "is_executing": false,
          "metadata": false,
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "rstories \u003d pd.read_csv(dir + \u0027rstories/rs_df.csv\u0027, sep\u003d\u0027|\u0027, index_col\u003d0)\n",
        "\n",
        "with open(dir + \u0027named_entities_all/rstories_ner.txt\u0027, \u0027r\u0027, encoding\u003d\"utf-8\") as myfile:\n",
        "        rstories_ner \u003d myfile.read().split(\u0027\\n\u0027)\n",
        "\n",
        "rstories_ner_split \u003d list()\n",
        "for ner in rstories_ner:\n",
        "    rstories_ner_split.append(ner.split(\u0027,\u0027))\n",
        "    \n",
        "rs_all_names \u003d list()\n",
        "for ner in rstories_ner_split:\n",
        "    rs_all_names.append(ner[0])\n",
        "\n",
        "rs_spelled_words \u003d set(nltk.word_tokenize(\u0027 \u0027.join(rs_all_names).lower()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "pycharm": {
          "is_executing": false,
          "metadata": false,
          "name": "#%%\n"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "text": [
            "1607 tags created...\n",
            "739 tags created...\n",
            "605 tags created...\n",
            "346 tags created...\n",
            "352 tags created...\n",
            "749 tags created...\n",
            "1465 tags created...\n"
          ],
          "output_type": "stream"
        }
      ],
      "source": "rs \u003d list(rstories[\u0027text\u0027])\nhandled_stories \u003d []\nfor story in rs:\n    handled_stories.append(contract_handle(story))"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "outputs": [],
      "source": "rstories[\u0027handled_text\u0027] \u003d handled_stories\nrstories.to_csv(dir + \u0027rstories/rs_df.csv\u0027, sep\u003d\u0027|\u0027)",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n",
          "is_executing": false
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": "rs_spell \u003d SpellChecker()\nrs_spell.word_frequency.load_words(rs_all_names)\nrs_spell.word_frequency.load_words(rs_spelled_words)\nrs \u003d list(rstories[\u0027text\u0027])\nfor story in rs:\n    words \u003d nltk.word_tokenize(story)\n    for word in words:\n        if str(word) !\u003d rs_spell.correction(str(word)):\n            print(word, rs_spell.correction(str(word)))",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n"
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-04-19T16:42:18.804955Z",
          "start_time": "2019-04-19T16:42:18.578789Z"
        },
        "pycharm": {
          "is_executing": false,
          "metadata": false,
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": "gravityfalls \u003d pd.read_csv(dir + \u0027gravityfalls/gf_eps.csv\u0027, sep\u003d\u0027|\u0027, index_col\u003d0)\n\nwith open(dir + \u0027named_entities_all/gravity_falls_ner.txt\u0027, \u0027r\u0027, encoding\u003d\"utf-8\") as myfile:\n        gravityfalls_ner \u003d myfile.read().split(\u0027\\n\u0027)\n\ngravityfalls_ner_split \u003d list()\nfor ner in gravityfalls_ner:\n    gravityfalls_ner_split.append(ner.split(\u0027,\u0027))\n\ngf_all_names \u003d list()\nfor ner in gravityfalls_ner_split:\n    gf_all_names.append(ner[0])\n\ngf_spelled_words \u003d set(nltk.word_tokenize(\u0027 \u0027.join(gf_all_names).lower()))"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": "gf \u003d list(gravityfalls[\u0027text\u0027])\nhandled_stories \u003d []\nfor story in gf:\n    handled_stories.append(contract_handle(story))",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n"
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": "gravityfalls[\u0027handled_text\u0027] \u003d handled_stories\ngravityfalls.to_csv(dir + \u0027gravityfalls/gf_eps.csv\u0027, sep\u003d\u0027|\u0027)",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n"
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "start_time": "2019-04-19T16:42:25.326Z"
        },
        "pycharm": {
          "is_executing": true,
          "metadata": false,
          "name": "#%%\n"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Beginning contraction handling\n",
            "Completed expansion\n",
            "6159 tags created...\n",
            "Beginning Spell Check\n",
            "Finished Spell Check\n",
            "Beginning contraction handling\n",
            "Completed expansion\n",
            "6619 tags created...\n",
            "Beginning Spell Check\n"
          ]
        }
      ],
      "source": "gf_spell \u003d SpellChecker()\ngf_spell.word_frequency.load_words(gf_spelled_words)\ncounter \u003d 0\nhandled_stories \u003d list(gravityfalls[\u0027handled_text\u0027])\nmispelled \u003d []\nfor story in handled_stories:\n    print(\u0027Beginning Spell Check\u0027)\n    words \u003d nltk.word_tokenize(story)\n    for word in words:\n        if str(word) !\u003d gf_spell.correction(str(word)):\n            counter +\u003d 1\n            mispelled.append(list(zip(str(word),gf_spell.correction(str(word)))))\n    print(\u0027Finished Spell Check\u0027)\nprint(counter)"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": "gf_spell \u003d SpellChecker()\ngf_spell.word_frequency.load_words(gf_spelled_words)\ncounter \u003d 0\nhandled_stories \u003d list(gravityfalls[\u0027handled_text\u0027])\nprint(\u0027Beginning Spell Check\u0027)\nwords \u003d nltk.word_tokenize(handled_stories[3])\nfor word in words:\n    if str(word) !\u003d gf_spell.correction(str(word)):\n        counter +\u003d 1\n        mispelled.append(list(zip(str(word),gf_spell.correction(str(word)))))\nprint(\u0027Finished Spell Check\u0027)\nprint(counter)\n",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n"
        }
      }
    }
  ],
  "metadata": {
    "hide_input": false,
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}