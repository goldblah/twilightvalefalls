{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#suppress warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "from pattern.en import suggest\n",
    "import pandas as pd\n",
    "from pycontractions import Contractions\n",
    "%run NER_Functions.py\n",
    "import re\n",
    "import nltk\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cont = Contractions(api_key=\"glove-twitter-25\")\n",
    "cont.load_models()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read in Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wtnv = pd.read_csv('episode_prelim_clean.csv', sep='|')\n",
    "texts = wtnv['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"wtnv_ner.txt\",\"r+\")  \n",
    "n = str(f.read())\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "named_entities = []\n",
    "bad = ['','Voiced Characters','You (character)','You (listener)','Antiques','Deer','Plastic Bags','Spiders','Them',\n",
    "      'Shame','LGBT characters','Mayors of Night Vale','Scientists of Night Vale']\n",
    "bad_start = ['The','Night','Abandoned','Antiques','Barista','Big','City','Coyote','Dark','Desert','Elementary','Haunted',\n",
    "            'Hidden','Hole','Juvenile','King','Mission','Museum','National','Old','Pine','Post','Pulsar','Radon','Red',\n",
    "            'Sand','Secret','Shambling','Skeleton','Stone','Used','Air-Filled','Apache','Barks','Cactus','Dr.','Enormous',\n",
    "            'Glow','Greater','Hierarchy','Hooded','Mr.','Mrs.','My','Otherworldly','Pastor','Prince','Sheriff',\n",
    "             'Soundproof','Station','Vague,','Whispering','White','World']\n",
    "lines = n.split('\\n')\n",
    "lines.sort()\n",
    "for ln in lines:\n",
    "    nam = ln.split('::')[0]\n",
    "    if nam not in bad:\n",
    "        if len(nam.split(' ')) > 1:\n",
    "            if nam.split(' ')[0] not in bad_start:\n",
    "                if \"'\" in nam.split(' ')[0]:\n",
    "                    named_entities.append(nam.split(' ')[0].split(\"'\")[0])\n",
    "                elif nam.split(' ')[0] == 'Intern':\n",
    "                    named_entities.append(nam.split(' ')[1])\n",
    "                else:\n",
    "                    named_entities.append(nam.split(' ')[0])\n",
    "        named_entities.append(nam.split('(')[0].strip().lower())\n",
    "        \n",
    "named_entities = list(set([n.lower() for n in named_entities]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_lengthening(text):\n",
    "    pattern = re.compile(r\"(.)\\1{2,}\")\n",
    "    return pattern.sub(r\"\\1\\1\", text)\n",
    "\n",
    "def clear_musics(text):\n",
    "    musics = re.findall(\"\\[“.*\\]\", text)\n",
    "    if len(musics) > 0:\n",
    "        mus = musics[0].strip().split(']')[0]+']'\n",
    "        text = text.replace(mus,'')\n",
    "        musics = re.findall(\"\\[“.*\\]\", text)\n",
    "        if len(musics) > 0:\n",
    "            mus = musics[0].split(']')[0]+']'\n",
    "            text = text.replace(mus,'')\n",
    "    \n",
    "    return text\n",
    "\n",
    "def clear_note(text):\n",
    "    if 'NOTE' in text:\n",
    "        if 'You can purchase' in text:\n",
    "            text = '1.INTRO.'+text.split('1.INTRO.')[1]\n",
    "        else:\n",
    "            text = text.split('page.]')[1]\n",
    "            \n",
    "    return text\n",
    "\n",
    "def clear_links(text):\n",
    "    if 'http' in text:\n",
    "        if 'Dinosaur emoji' in text:\n",
    "            text = text.split('Dinosaur emoji')[0]\n",
    "        else:\n",
    "            if 'Mucca' in text:\n",
    "                text = text.replace(\"[Weather: “J'Accuse” by Mucca Pazza. http://www.muccapazza.com]\",'')\n",
    "            else:\n",
    "                if 'Mal' in text:\n",
    "                    text = text.replace(\"“See Me” by Mal Blum https://malblum.bandcamp.com/ [I couldn’t find this song on there, but check out the rest of their stuff]\",'')\n",
    "    return text\n",
    "\n",
    "def clear_bad(text):\n",
    "    bad = ['for this episode', 'throughout the transcript', 'cecilspeaks', \n",
    "           \"Transcriber's\", 'transcriber','transcribe']\n",
    "    for sub in bad:\n",
    "        if sub in text:\n",
    "            if sub == bad[0]:\n",
    "                text = 'We'+text.split('] We')[1]\n",
    "            elif sub == bad[1]:\n",
    "                text = 'Act'+text.split('] Act')[1]\n",
    "            elif sub == bad[2]:\n",
    "                if ('@cecilbaldwin-fan' in text) & ('iTunes' in text):\n",
    "                    text = text.split('as Earl Harlan.Enjoy. ')[1].split('bye Earl. ')[0]+'bye Earl.'\n",
    "                else:\n",
    "                    text = ''\n",
    "            elif sub == bad[3]:\n",
    "                text = text.split('headphones] ')[1]\n",
    "            elif sub == bad[4]:\n",
    "                text = text.split(' Joseph Fink: Alice')[0].split(' Clip from')[0]\n",
    "            elif sub == bad[5]:\n",
    "                if 'following transcript' in text:\n",
    "                    text = text.split('links] ')[1]\n",
    "                else:\n",
    "                    if 'except for the parts that are.  ' in text:\n",
    "                        text = text.split('except for the parts that are.  ')[1]\n",
    "                \n",
    "    return text\n",
    "\n",
    "def remove_notes(text):\n",
    "    s = text\n",
    "    op = s.find('[')\n",
    "    clo = s.find(']')\n",
    "    while op != -1:\n",
    "        s = s.replace(s[op:clo+1],'')\n",
    "        op = s.find('[')\n",
    "        clo = s.find(']')\n",
    "    return s.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spellcheck(st,f):\n",
    "    fixes = f\n",
    "    \n",
    "    pos_toks = NER_fix(named_entities,[st.lower()])\n",
    "    all_toks = []\n",
    "    for tok in pos_toks[0]:\n",
    "        all_toks.append(tok[0])\n",
    "\n",
    "    toks = list(set(all_toks))\n",
    "    print('%d tokens created...reduced to %d...' %(len(all_toks),len(toks)))\n",
    "    \n",
    "    for fix in fixes:\n",
    "        #print('Applying fix: ',fix)\n",
    "        target = fix[0]\n",
    "        while target in all_toks:\n",
    "            ind = all_toks.index(fix[0])\n",
    "            all_toks[ind] = fix[1]\n",
    "    print('Applied preexisting fixes...')\n",
    "    \n",
    "    d = {}\n",
    "    \n",
    "    for tok in toks:\n",
    "        d[tok] = suggest(tok)\n",
    "        \n",
    "    print('Initial dictionary created...')\n",
    "    \n",
    "    d2 = dict(d)\n",
    "    for word in d2:\n",
    "        sugs = d2[word]\n",
    "\n",
    "        if (len(sugs) == 1) and (sugs[0][1] == 1.0) and (sugs[0][0] == word):\n",
    "            if word in d:\n",
    "                del d[word]\n",
    "            \n",
    "    print('Properly spelled words removed...%d tokens remaining...' %len(d))\n",
    "\n",
    "    d2 = dict(d)\n",
    "    for word in d2:\n",
    "        sugs = d2[word]\n",
    "\n",
    "        if (word == 'arby') or (word in named_entities) or (word == 'chen'):\n",
    "            if word in d:\n",
    "                del d[word]\n",
    "\n",
    "        elif sugs[0][1] == 1.0:\n",
    "            fixes.append((word,sugs[0][0]))\n",
    "            if word in d:\n",
    "                del d[word]\n",
    "            \n",
    "    print('Easy fixes removed...%d tokens remaining...' %len(d))\n",
    "    \n",
    "    d2 = dict(d)\n",
    "    for word in d2:\n",
    "        first_two = word[:2]\n",
    "        last_two = word[-2:]\n",
    "        sugsf1 = suggest(first_two)\n",
    "\n",
    "        if (len(sugsf1) == 1) and (sugsf1[0][1] == 1.0) and (sugsf1[0][0] == first_two):\n",
    "            sugsf2 = suggest(word[2:])\n",
    "            if (len(sugsf2) == 1) and (sugsf2[0][1] == 1.0) and (sugsf2[0][0] == word[2:]):\n",
    "                fixes.append((word,sugsf1[0][0]+' '+sugsf2[0][0]))\n",
    "                if word in d:\n",
    "                    del d[word]\n",
    "\n",
    "        sugsl1 = suggest(last_two)\n",
    "        if (len(sugsl1) == 1) and (sugsl1[0][1] == 1.0) and (sugsl1[0][0] == last_two):\n",
    "            sugsl2 = suggest(word[:-2])\n",
    "            if (len(sugsl2) == 1) and (sugsl2[0][1] == 1.0) and (sugsl2[0][0] == word[:-2]):\n",
    "                fixes.append((word,sugsl2[0][0]+' '+sugsl1[0][0]))\n",
    "                if word in d:\n",
    "                    del d[word]\n",
    "                \n",
    "    print('Two letter mash-ups removed...%d tokens remaining...' %len(d))\n",
    "    \n",
    "    d2 = dict(d)\n",
    "    for word in d2:\n",
    "        first_three = word[:3]\n",
    "        last_three = word[-3:]\n",
    "        sugsf1 = suggest(first_three)\n",
    "\n",
    "        if (len(sugsf1) == 1) and (sugsf1[0][1] == 1.0) and (sugsf1[0][0] == first_three):\n",
    "            sugsf2 = suggest(word[3:])\n",
    "            if (len(sugsf2) == 1) and (sugsf2[0][1] == 1.0) and (sugsf2[0][0] == word[3:]):\n",
    "                fixes.append((word,sugsf1[0][0]+' '+sugsf2[0][0]))\n",
    "                if word in d:\n",
    "                    del d[word]\n",
    "\n",
    "        sugsl1 = suggest(last_three)\n",
    "        if (len(sugsl1) == 1) and (sugsl1[0][1] == 1.0) and (sugsl1[0][0] == last_three):\n",
    "            sugsl2 = suggest(word[:-3])\n",
    "            if (len(sugsl2) == 1) and (sugsl2[0][1] == 1.0) and (sugsl2[0][0] == word[:-3]):\n",
    "                fixes.append((word,sugsl2[0][0]+' '+sugsl1[0][0]))\n",
    "                if word in d:\n",
    "                    del d[word]\n",
    "                \n",
    "    print('Three letter mash-ups removed...%d tokens remaining...' %len(d))\n",
    "    \n",
    "    d2 = dict(d)\n",
    "    for word in d2:\n",
    "        sugs = d2[word]\n",
    "\n",
    "        if sugs[0][1] >= 0.75:\n",
    "            fixes.append((word,sugs[0][0]))\n",
    "            if word in d:\n",
    "                del d[word]\n",
    "            \n",
    "    print('Okay fixes removed...%d tokens remaining...' %len(d))\n",
    "    \n",
    "    d2 = dict(d)\n",
    "    for word in d2:\n",
    "        good = False\n",
    "        word1 = reduce_lengthening(word)\n",
    "        if word != word1:\n",
    "            sug = suggest(word1)\n",
    "            if (sug[0][1] >= 0.75):\n",
    "                fixes.append((word,sug[0][0]))\n",
    "                if word in d:\n",
    "                    del d[word]\n",
    "                \n",
    "    print('Elongated fixes removed...%d tokens remaining...' %len(d))\n",
    "    \n",
    "    if len(d) <= 100:\n",
    "        d2 = dict(d)\n",
    "        for word in d2:\n",
    "            good = False\n",
    "            word1 = reduce_lengthening(word)\n",
    "            for i in range(1,len(word)):\n",
    "                if good == False:\n",
    "                    start = word[0:i]\n",
    "                    end = word[i:]\n",
    "\n",
    "                    sug_s = suggest(start)\n",
    "                    if (len(sug_s) == 1) and (sug_s[0][1] == 1.0) and (sug_s[0][0] == start):\n",
    "                        sug_e = suggest(end)\n",
    "                        if (len(sug_e) == 1) and (sug_e[0][1] == 1.0) and (sug_e[0][0] == end):\n",
    "                            fixes.append((word,start+' '+end))\n",
    "                            if word in d:\n",
    "                                del d[word]\n",
    "                            good = True\n",
    "                        \n",
    "    print('Remaining mash-ups removed...%d tokens remaining...' %len(d))\n",
    "    \n",
    "    for fix in fixes:\n",
    "        #print('Applying fix: ',fix)\n",
    "        target = fix[0]\n",
    "        while target in all_toks:\n",
    "            ind = all_toks.index(fix[0])\n",
    "            all_toks[ind] = fix[1]\n",
    "            \n",
    "    print('Tokens fixed...')\n",
    "    \n",
    "    t = ' '.join(all_toks)\n",
    "    \n",
    "    return t,fixes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def contract_handle(st):\n",
    "    #print('Doing text %d...' %texts.index(st))\n",
    "    t = list(cont.expand_texts([st.replace(\"’\",\"'\")]))[0]\n",
    "    tags = nltk.pos_tag(word_tokenize(str(t)))\n",
    "    temp = []\n",
    "    #print('%d tags created...' %len(tags))\n",
    "    for tag in tags:\n",
    "        temp.append(tag[0])\n",
    "            \n",
    "    return ' '.join(temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [clear_links(clear_note(clear_musics(t.replace('\\xa0',' ').replace('…','...').replace('’',\"'\")))).strip() for t in texts]\n",
    "texts = [clear_bad(t.replace('“','\"').replace('”','\"').replace('\\t',' ').replace('\\n',' ')) for t in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts[135] = texts[135]+']'\n",
    "texts[141] = texts[141]+']'\n",
    "line = texts[154]\n",
    "index = line.find('clears throat]')\n",
    "texts[154] = line[:index] + '[ ' + line[index:]\n",
    "texts[154] = texts[154]+']'\n",
    "texts[156] = texts[156]+']'\n",
    "texts[163] = texts[163]+']'\n",
    "texts[12] = texts[12].replace('Translations : ( 1 ) You are in danger . ( 2 ) they are coming . ( 3 ) They will come from below . Pies will not help . Russian phrases pieced together using information from the Welcome to Night Vale Wiki , and adding minor variations in Google Translate until the audio preview on the site sounded , to my ear , more or less like what I was hearing in the episode . Then solorwind was kind enough to send me the actual phrases . Any errors are 100 % my fault .','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [remove_notes(t) for t in texts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SpellCheck"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fixes = [('willbe','will be')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(texts)):\n",
    "    print('Processing Text %d of %d...' %(i+1,len(texts)))\n",
    "    print('****************************************************************')\n",
    "    texts[i],fixes = spellcheck(texts[i],fixes)\n",
    "    print('Length fixes: ',len(fixes))\n",
    "    print('****************************************************************')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contraction Handler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "texts = [contract_handle(te.replace(\" ' \",\"'\")) for te in texts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataFrame (okay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wtnv['text'] = texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wtnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#wtnv = wtnv.reset_index()\n",
    "wtnv_2 = pd.DataFrame()\n",
    "wtnv_2['Episode'] = wtnv['episode_name']\n",
    "wtnv_2['Text'] = wtnv['text']\n",
    "wtnv_2['Source'] = 'WTNV'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wtnv_2.to_csv(r'C:\\Users\\cml4603\\Desktop\\twilightvalefalls\\wtnv_cleaned.csv', sep='|')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
