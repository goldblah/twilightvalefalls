{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#suppress warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Load LSTM network and generate text\n",
    "import sys\n",
    "import numpy\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,Dropout,LSTM\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils import np_utils\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_weights(raw_text,seq_length,epoch_num,num,verbose=0):\n",
    "    #get chars in raw_text\n",
    "    chars = sorted(list(set(raw_text)))\n",
    "    \n",
    "    #get mapping of char to int value and vice versa\n",
    "    char_to_int = dict((c, i) for i, c in enumerate(chars))\n",
    "    int_to_char = dict((i, c) for i, c in enumerate(chars))\n",
    "    \n",
    "    #print out summary of data\n",
    "    n_chars = len(raw_text)\n",
    "    n_vocab = len(chars)\n",
    "    print(\"Total Characters: \", n_chars)\n",
    "    print(\"Total Vocab (Unique Characters): \", n_vocab)\n",
    "    \n",
    "    #get X and Y of data (sequence in and next character)\n",
    "    #X will be a sequence of characters with length seq_length (given)\n",
    "    #Y will be the corresponding next character\n",
    "    dataX = []\n",
    "    dataY = []\n",
    "    for i in range(0, n_chars - seq_length, 1):\n",
    "        seq_in = raw_text[i:i + seq_length]\n",
    "        seq_out = raw_text[i + seq_length]\n",
    "        dataX.append([char_to_int[char] for char in seq_in])\n",
    "        dataY.append(char_to_int[seq_out])\n",
    "    n_patterns = len(dataX)\n",
    "    #num of patters is the number of text sequences of length seq_length we have\n",
    "    print(\"Total Patterns: \", n_patterns,'\\n')\n",
    "    \n",
    "    # reshape X to be [samples, time steps, features]\n",
    "    X = numpy.reshape(dataX, (n_patterns, seq_length, 1))\n",
    "    \n",
    "    # normalize X (really simple scaler)\n",
    "    X = X / float(n_vocab)\n",
    "    \n",
    "    # do one hot encoding on the output variable\n",
    "    y = np_utils.to_categorical(dataY)\n",
    "    \n",
    "    # define the LSTM model\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2])))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(y.shape[1], activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "    \n",
    "    # define the checkpoint, make file names standard\n",
    "    filepath=\"weights-improvement-{epoch:02d}-{loss:.4f}.hdf5\"\n",
    "    checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "    callbacks_list = [checkpoint]\n",
    "    \n",
    "    # fit the model\n",
    "    model.fit(X, y, epochs=epoch_num, batch_size=128, callbacks=callbacks_list)\n",
    "    \n",
    "    # define the LSTM model - part 2\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2])))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(y.shape[1], activation='softmax'))\n",
    "    \n",
    "    #get latest file (newest weight)\n",
    "    list_of_files = glob.glob('../twilightvalefalls/*.hdf5')\n",
    "    latest_file = max(list_of_files, key=os.path.getctime)\n",
    "    \n",
    "    # load the network weights\n",
    "    filename = latest_file\n",
    "    model.load_weights(filename)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "    \n",
    "    # pick a random seed num times (new one for each text generated)\n",
    "    seeds = []\n",
    "    for n in range(num):\n",
    "        start = numpy.random.randint(0, len(dataX)-1)\n",
    "        pattern = dataX[start]\n",
    "        s = ''.join([int_to_char[value] for value in pattern])\n",
    "        if verbose > 0:\n",
    "            print(\"Seed #%d:\" %(n+1))\n",
    "            print(\"\\\"\", s, \"\\\"\")\n",
    "        seeds.append(pattern)\n",
    "        \n",
    "    gens = []\n",
    "    for seed in seeds:\n",
    "        pattern = seed\n",
    "        # generate characters\n",
    "        for i in range(seq_length):\n",
    "            x = numpy.reshape(pattern, (1, len(pattern), 1))\n",
    "            x = x / float(n_vocab)\n",
    "            prediction = model.predict(x, verbose=0)\n",
    "            index = numpy.argmax(prediction)\n",
    "            result = int_to_char[index]\n",
    "            seq_in = [int_to_char[value] for value in pattern]\n",
    "            pattern.append(index)\n",
    "            pattern = pattern[1:len(pattern)]\n",
    "            \n",
    "        gens.append(pattern)\n",
    "        \n",
    "    for text in gens:\n",
    "        if verbose > 0:\n",
    "            print(''.join([int_to_char[value] for value in text]))\n",
    "        \n",
    "    return [''.join([int_to_char[value] for value in text]) for text in gens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RStories RNN\n",
    "txt_files = glob.glob(\"rStories/*.txt\")\n",
    "rstories_fulltext = ''\n",
    "for f in txt_files:\n",
    "    file = open(f,encoding='utf-8') \n",
    "    st = file.read() \n",
    "    rstories_fulltext = rstories_fulltext + ' ' + st\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "rstories_fulltext = rstories_fulltext.replace('\\n',' ').replace('        ','').strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Characters:  26885\n",
      "Total Vocab (Unique Characters):  66\n",
      "Total Patterns:  26785 \n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\cml4603\\AppData\\Local\\Continuum\\anaconda3\\envs\\nlp\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From C:\\Users\\cml4603\\AppData\\Local\\Continuum\\anaconda3\\envs\\nlp\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From C:\\Users\\cml4603\\AppData\\Local\\Continuum\\anaconda3\\envs\\nlp\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/200\n",
      "26785/26785 [==============================] - 74s 3ms/step - loss: 3.1331\n",
      "\n",
      "Epoch 00001: loss improved from inf to 3.13311, saving model to weights-improvement-01-3.1331.hdf5\n",
      "Epoch 2/200\n",
      "26785/26785 [==============================] - 72s 3ms/step - loss: 3.0736\n",
      "\n",
      "Epoch 00002: loss improved from 3.13311 to 3.07360, saving model to weights-improvement-02-3.0736.hdf5\n",
      "Epoch 3/200\n",
      "26785/26785 [==============================] - 74s 3ms/step - loss: 3.0390\n",
      "\n",
      "Epoch 00003: loss improved from 3.07360 to 3.03900, saving model to weights-improvement-03-3.0390.hdf5\n",
      "Epoch 4/200\n",
      "26785/26785 [==============================] - 72s 3ms/step - loss: 2.9461\n",
      "\n",
      "Epoch 00004: loss improved from 3.03900 to 2.94614, saving model to weights-improvement-04-2.9461.hdf5\n",
      "Epoch 5/200\n",
      "26785/26785 [==============================] - 72s 3ms/step - loss: 2.8964\n",
      "\n",
      "Epoch 00005: loss improved from 2.94614 to 2.89636, saving model to weights-improvement-05-2.8964.hdf5\n",
      "Epoch 6/200\n",
      "26785/26785 [==============================] - 71s 3ms/step - loss: 2.8726\n",
      "\n",
      "Epoch 00006: loss improved from 2.89636 to 2.87258, saving model to weights-improvement-06-2.8726.hdf5\n",
      "Epoch 7/200\n",
      "26785/26785 [==============================] - 72s 3ms/step - loss: 2.8504\n",
      "\n",
      "Epoch 00007: loss improved from 2.87258 to 2.85037, saving model to weights-improvement-07-2.8504.hdf5\n",
      "Epoch 8/200\n",
      "26785/26785 [==============================] - 72s 3ms/step - loss: 2.8319\n",
      "\n",
      "Epoch 00008: loss improved from 2.85037 to 2.83191, saving model to weights-improvement-08-2.8319.hdf5\n",
      "Epoch 9/200\n",
      "26785/26785 [==============================] - 72s 3ms/step - loss: 2.8161\n",
      "\n",
      "Epoch 00009: loss improved from 2.83191 to 2.81614, saving model to weights-improvement-09-2.8161.hdf5\n",
      "Epoch 10/200\n",
      "26785/26785 [==============================] - 72s 3ms/step - loss: 2.8080\n",
      "\n",
      "Epoch 00010: loss improved from 2.81614 to 2.80799, saving model to weights-improvement-10-2.8080.hdf5\n",
      "Epoch 11/200\n",
      "26785/26785 [==============================] - 73s 3ms/step - loss: 2.7911\n",
      "\n",
      "Epoch 00011: loss improved from 2.80799 to 2.79109, saving model to weights-improvement-11-2.7911.hdf5\n",
      "Epoch 12/200\n",
      "26785/26785 [==============================] - 73s 3ms/step - loss: 2.7790\n",
      "\n",
      "Epoch 00012: loss improved from 2.79109 to 2.77904, saving model to weights-improvement-12-2.7790.hdf5\n",
      "Epoch 13/200\n",
      "26785/26785 [==============================] - 72s 3ms/step - loss: 2.7643\n",
      "\n",
      "Epoch 00013: loss improved from 2.77904 to 2.76433, saving model to weights-improvement-13-2.7643.hdf5\n",
      "Epoch 14/200\n",
      "26785/26785 [==============================] - 72s 3ms/step - loss: 2.7532\n",
      "\n",
      "Epoch 00014: loss improved from 2.76433 to 2.75318, saving model to weights-improvement-14-2.7532.hdf5\n",
      "Epoch 15/200\n",
      "26785/26785 [==============================] - 72s 3ms/step - loss: 2.7391\n",
      "\n",
      "Epoch 00015: loss improved from 2.75318 to 2.73911, saving model to weights-improvement-15-2.7391.hdf5\n",
      "Epoch 16/200\n",
      "26785/26785 [==============================] - 72s 3ms/step - loss: 2.7296\n",
      "\n",
      "Epoch 00016: loss improved from 2.73911 to 2.72956, saving model to weights-improvement-16-2.7296.hdf5\n",
      "Epoch 17/200\n",
      "26785/26785 [==============================] - 76s 3ms/step - loss: 2.7160\n",
      "\n",
      "Epoch 00017: loss improved from 2.72956 to 2.71599, saving model to weights-improvement-17-2.7160.hdf5\n",
      "Epoch 18/200\n",
      "26785/26785 [==============================] - 76s 3ms/step - loss: 2.7040\n",
      "\n",
      "Epoch 00018: loss improved from 2.71599 to 2.70399, saving model to weights-improvement-18-2.7040.hdf5\n",
      "Epoch 19/200\n",
      "26785/26785 [==============================] - 75s 3ms/step - loss: 2.6897\n",
      "\n",
      "Epoch 00019: loss improved from 2.70399 to 2.68966, saving model to weights-improvement-19-2.6897.hdf5\n",
      "Epoch 20/200\n",
      "26785/26785 [==============================] - 75s 3ms/step - loss: 2.6763\n",
      "\n",
      "Epoch 00020: loss improved from 2.68966 to 2.67632, saving model to weights-improvement-20-2.6763.hdf5\n",
      "Epoch 21/200\n",
      "26785/26785 [==============================] - 73s 3ms/step - loss: 2.6593\n",
      "\n",
      "Epoch 00021: loss improved from 2.67632 to 2.65931, saving model to weights-improvement-21-2.6593.hdf5\n",
      "Epoch 22/200\n",
      "26785/26785 [==============================] - 73s 3ms/step - loss: 2.6456\n",
      "\n",
      "Epoch 00022: loss improved from 2.65931 to 2.64558, saving model to weights-improvement-22-2.6456.hdf5\n",
      "Epoch 23/200\n",
      "26785/26785 [==============================] - 73s 3ms/step - loss: 2.6263\n",
      "\n",
      "Epoch 00023: loss improved from 2.64558 to 2.62628, saving model to weights-improvement-23-2.6263.hdf5\n",
      "Epoch 24/200\n",
      "26785/26785 [==============================] - 73s 3ms/step - loss: 2.6091\n",
      "\n",
      "Epoch 00024: loss improved from 2.62628 to 2.60910, saving model to weights-improvement-24-2.6091.hdf5\n",
      "Epoch 25/200\n",
      "26785/26785 [==============================] - 76s 3ms/step - loss: 2.5877\n",
      "\n",
      "Epoch 00025: loss improved from 2.60910 to 2.58771, saving model to weights-improvement-25-2.5877.hdf5\n",
      "Epoch 26/200\n",
      "26785/26785 [==============================] - 74s 3ms/step - loss: 2.5637\n",
      "\n",
      "Epoch 00026: loss improved from 2.58771 to 2.56371, saving model to weights-improvement-26-2.5637.hdf5\n",
      "Epoch 27/200\n",
      "26785/26785 [==============================] - 100s 4ms/step - loss: 2.5398\n",
      "\n",
      "Epoch 00027: loss improved from 2.56371 to 2.53984, saving model to weights-improvement-27-2.5398.hdf5\n",
      "Epoch 28/200\n",
      "26785/26785 [==============================] - 90s 3ms/step - loss: 2.5136\n",
      "\n",
      "Epoch 00028: loss improved from 2.53984 to 2.51360, saving model to weights-improvement-28-2.5136.hdf5\n",
      "Epoch 29/200\n",
      "26785/26785 [==============================] - 70s 3ms/step - loss: 2.4874\n",
      "\n",
      "Epoch 00029: loss improved from 2.51360 to 2.48741, saving model to weights-improvement-29-2.4874.hdf5\n",
      "Epoch 30/200\n",
      "26785/26785 [==============================] - 72s 3ms/step - loss: 2.4564\n",
      "\n",
      "Epoch 00030: loss improved from 2.48741 to 2.45639, saving model to weights-improvement-30-2.4564.hdf5\n",
      "Epoch 31/200\n",
      "26785/26785 [==============================] - 71s 3ms/step - loss: 2.4311\n",
      "\n",
      "Epoch 00031: loss improved from 2.45639 to 2.43111, saving model to weights-improvement-31-2.4311.hdf5\n",
      "Epoch 32/200\n",
      "26785/26785 [==============================] - 72s 3ms/step - loss: 2.4007\n",
      "\n",
      "Epoch 00032: loss improved from 2.43111 to 2.40067, saving model to weights-improvement-32-2.4007.hdf5\n",
      "Epoch 33/200\n",
      "26785/26785 [==============================] - 72s 3ms/step - loss: 2.3615\n",
      "\n",
      "Epoch 00033: loss improved from 2.40067 to 2.36147, saving model to weights-improvement-33-2.3615.hdf5\n",
      "Epoch 34/200\n",
      "26785/26785 [==============================] - 72s 3ms/step - loss: 2.3314\n",
      "\n",
      "Epoch 00034: loss improved from 2.36147 to 2.33136, saving model to weights-improvement-34-2.3314.hdf5\n",
      "Epoch 35/200\n",
      "26785/26785 [==============================] - 72s 3ms/step - loss: 2.3014\n",
      "\n",
      "Epoch 00035: loss improved from 2.33136 to 2.30143, saving model to weights-improvement-35-2.3014.hdf5\n",
      "Epoch 36/200\n",
      "26785/26785 [==============================] - 73s 3ms/step - loss: 2.2686\n",
      "\n",
      "Epoch 00036: loss improved from 2.30143 to 2.26861, saving model to weights-improvement-36-2.2686.hdf5\n",
      "Epoch 37/200\n",
      "26785/26785 [==============================] - 72s 3ms/step - loss: 2.2358\n",
      "\n",
      "Epoch 00037: loss improved from 2.26861 to 2.23584, saving model to weights-improvement-37-2.2358.hdf5\n",
      "Epoch 38/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26785/26785 [==============================] - 72s 3ms/step - loss: 2.1973\n",
      "\n",
      "Epoch 00038: loss improved from 2.23584 to 2.19726, saving model to weights-improvement-38-2.1973.hdf5\n",
      "Epoch 39/200\n",
      "26785/26785 [==============================] - 73s 3ms/step - loss: 2.1653\n",
      "\n",
      "Epoch 00039: loss improved from 2.19726 to 2.16530, saving model to weights-improvement-39-2.1653.hdf5\n",
      "Epoch 40/200\n",
      "26785/26785 [==============================] - 72s 3ms/step - loss: 2.1259\n",
      "\n",
      "Epoch 00040: loss improved from 2.16530 to 2.12591, saving model to weights-improvement-40-2.1259.hdf5\n",
      "Epoch 41/200\n",
      "26785/26785 [==============================] - 72s 3ms/step - loss: 2.0932\n",
      "\n",
      "Epoch 00041: loss improved from 2.12591 to 2.09317, saving model to weights-improvement-41-2.0932.hdf5\n",
      "Epoch 42/200\n",
      "26785/26785 [==============================] - 72s 3ms/step - loss: 2.0677\n",
      "\n",
      "Epoch 00042: loss improved from 2.09317 to 2.06771, saving model to weights-improvement-42-2.0677.hdf5\n",
      "Epoch 43/200\n",
      "26785/26785 [==============================] - 72s 3ms/step - loss: 2.0308\n",
      "\n",
      "Epoch 00043: loss improved from 2.06771 to 2.03084, saving model to weights-improvement-43-2.0308.hdf5\n",
      "Epoch 44/200\n",
      "26785/26785 [==============================] - 72s 3ms/step - loss: 2.0036\n",
      "\n",
      "Epoch 00044: loss improved from 2.03084 to 2.00364, saving model to weights-improvement-44-2.0036.hdf5\n",
      "Epoch 45/200\n",
      "26785/26785 [==============================] - 73s 3ms/step - loss: 1.9766\n",
      "\n",
      "Epoch 00045: loss improved from 2.00364 to 1.97661, saving model to weights-improvement-45-1.9766.hdf5\n",
      "Epoch 46/200\n",
      "26785/26785 [==============================] - 72s 3ms/step - loss: 1.9370\n",
      "\n",
      "Epoch 00046: loss improved from 1.97661 to 1.93698, saving model to weights-improvement-46-1.9370.hdf5\n",
      "Epoch 47/200\n",
      "26785/26785 [==============================] - 72s 3ms/step - loss: 1.9135\n",
      "\n",
      "Epoch 00047: loss improved from 1.93698 to 1.91346, saving model to weights-improvement-47-1.9135.hdf5\n",
      "Epoch 48/200\n",
      "26785/26785 [==============================] - 72s 3ms/step - loss: 1.8735\n",
      "\n",
      "Epoch 00048: loss improved from 1.91346 to 1.87349, saving model to weights-improvement-48-1.8735.hdf5\n",
      "Epoch 49/200\n",
      "26785/26785 [==============================] - 73s 3ms/step - loss: 1.8537\n",
      "\n",
      "Epoch 00049: loss improved from 1.87349 to 1.85369, saving model to weights-improvement-49-1.8537.hdf5\n",
      "Epoch 50/200\n",
      "26785/26785 [==============================] - 73s 3ms/step - loss: 1.8358\n",
      "\n",
      "Epoch 00050: loss improved from 1.85369 to 1.83582, saving model to weights-improvement-50-1.8358.hdf5\n",
      "Epoch 51/200\n",
      "26785/26785 [==============================] - 73s 3ms/step - loss: 1.7929\n",
      "\n",
      "Epoch 00051: loss improved from 1.83582 to 1.79291, saving model to weights-improvement-51-1.7929.hdf5\n",
      "Epoch 52/200\n",
      "26785/26785 [==============================] - 73s 3ms/step - loss: 1.7716\n",
      "\n",
      "Epoch 00052: loss improved from 1.79291 to 1.77156, saving model to weights-improvement-52-1.7716.hdf5\n",
      "Epoch 53/200\n",
      "26785/26785 [==============================] - 73s 3ms/step - loss: 1.7469\n",
      "\n",
      "Epoch 00053: loss improved from 1.77156 to 1.74695, saving model to weights-improvement-53-1.7469.hdf5\n",
      "Epoch 54/200\n",
      "26785/26785 [==============================] - 73s 3ms/step - loss: 1.7151\n",
      "\n",
      "Epoch 00054: loss improved from 1.74695 to 1.71507, saving model to weights-improvement-54-1.7151.hdf5\n",
      "Epoch 55/200\n",
      "26785/26785 [==============================] - 73s 3ms/step - loss: 1.6880\n",
      "\n",
      "Epoch 00055: loss improved from 1.71507 to 1.68801, saving model to weights-improvement-55-1.6880.hdf5\n",
      "Epoch 56/200\n",
      "26785/26785 [==============================] - 73s 3ms/step - loss: 1.6729\n",
      "\n",
      "Epoch 00056: loss improved from 1.68801 to 1.67288, saving model to weights-improvement-56-1.6729.hdf5\n",
      "Epoch 57/200\n",
      "26785/26785 [==============================] - 73s 3ms/step - loss: 1.6534\n",
      "\n",
      "Epoch 00057: loss improved from 1.67288 to 1.65344, saving model to weights-improvement-57-1.6534.hdf5\n",
      "Epoch 58/200\n",
      "26785/26785 [==============================] - 75s 3ms/step - loss: 1.6166\n",
      "\n",
      "Epoch 00058: loss improved from 1.65344 to 1.61655, saving model to weights-improvement-58-1.6166.hdf5\n",
      "Epoch 59/200\n",
      "26785/26785 [==============================] - 73s 3ms/step - loss: 1.6121\n",
      "\n",
      "Epoch 00059: loss improved from 1.61655 to 1.61209, saving model to weights-improvement-59-1.6121.hdf5\n",
      "Epoch 60/200\n",
      "26785/26785 [==============================] - 74s 3ms/step - loss: 1.6083\n",
      "\n",
      "Epoch 00060: loss improved from 1.61209 to 1.60825, saving model to weights-improvement-60-1.6083.hdf5\n",
      "Epoch 61/200\n",
      "26785/26785 [==============================] - 74s 3ms/step - loss: 1.5682\n",
      "\n",
      "Epoch 00061: loss improved from 1.60825 to 1.56815, saving model to weights-improvement-61-1.5682.hdf5\n",
      "Epoch 62/200\n",
      "26785/26785 [==============================] - 74s 3ms/step - loss: 1.5378\n",
      "\n",
      "Epoch 00062: loss improved from 1.56815 to 1.53783, saving model to weights-improvement-62-1.5378.hdf5\n",
      "Epoch 63/200\n",
      "26785/26785 [==============================] - 74s 3ms/step - loss: 1.5172\n",
      "\n",
      "Epoch 00063: loss improved from 1.53783 to 1.51722, saving model to weights-improvement-63-1.5172.hdf5\n",
      "Epoch 64/200\n",
      "26785/26785 [==============================] - 74s 3ms/step - loss: 1.4913\n",
      "\n",
      "Epoch 00064: loss improved from 1.51722 to 1.49134, saving model to weights-improvement-64-1.4913.hdf5\n",
      "Epoch 65/200\n",
      "26785/26785 [==============================] - 74s 3ms/step - loss: 1.4707\n",
      "\n",
      "Epoch 00065: loss improved from 1.49134 to 1.47069, saving model to weights-improvement-65-1.4707.hdf5\n",
      "Epoch 66/200\n",
      "26785/26785 [==============================] - 75s 3ms/step - loss: 1.4556\n",
      "\n",
      "Epoch 00066: loss improved from 1.47069 to 1.45559, saving model to weights-improvement-66-1.4556.hdf5\n",
      "Epoch 67/200\n",
      "26785/26785 [==============================] - 74s 3ms/step - loss: 1.4477\n",
      "\n",
      "Epoch 00067: loss improved from 1.45559 to 1.44770, saving model to weights-improvement-67-1.4477.hdf5\n",
      "Epoch 68/200\n",
      "26785/26785 [==============================] - 74s 3ms/step - loss: 1.4300\n",
      "\n",
      "Epoch 00068: loss improved from 1.44770 to 1.42999, saving model to weights-improvement-68-1.4300.hdf5\n",
      "Epoch 69/200\n",
      "26785/26785 [==============================] - 73s 3ms/step - loss: 1.4135\n",
      "\n",
      "Epoch 00069: loss improved from 1.42999 to 1.41350, saving model to weights-improvement-69-1.4135.hdf5\n",
      "Epoch 70/200\n",
      "26785/26785 [==============================] - 75s 3ms/step - loss: 1.4021\n",
      "\n",
      "Epoch 00070: loss improved from 1.41350 to 1.40206, saving model to weights-improvement-70-1.4021.hdf5\n",
      "Epoch 71/200\n",
      "26785/26785 [==============================] - 74s 3ms/step - loss: 1.3778\n",
      "\n",
      "Epoch 00071: loss improved from 1.40206 to 1.37784, saving model to weights-improvement-71-1.3778.hdf5\n",
      "Epoch 72/200\n",
      "26785/26785 [==============================] - 77s 3ms/step - loss: 1.3633\n",
      "\n",
      "Epoch 00072: loss improved from 1.37784 to 1.36329, saving model to weights-improvement-72-1.3633.hdf5\n",
      "Epoch 73/200\n",
      "26785/26785 [==============================] - 76s 3ms/step - loss: 1.3364\n",
      "\n",
      "Epoch 00073: loss improved from 1.36329 to 1.33641, saving model to weights-improvement-73-1.3364.hdf5\n",
      "Epoch 74/200\n",
      "26785/26785 [==============================] - 75s 3ms/step - loss: 1.3378\n",
      "\n",
      "Epoch 00074: loss did not improve from 1.33641\n",
      "Epoch 75/200\n",
      "26785/26785 [==============================] - 74s 3ms/step - loss: 1.3260\n",
      "\n",
      "Epoch 00075: loss improved from 1.33641 to 1.32600, saving model to weights-improvement-75-1.3260.hdf5\n",
      "Epoch 76/200\n",
      "26785/26785 [==============================] - 73s 3ms/step - loss: 1.2886\n",
      "\n",
      "Epoch 00076: loss improved from 1.32600 to 1.28864, saving model to weights-improvement-76-1.2886.hdf5\n",
      "Epoch 77/200\n",
      "26785/26785 [==============================] - 74s 3ms/step - loss: 1.2842\n",
      "\n",
      "Epoch 00077: loss improved from 1.28864 to 1.28419, saving model to weights-improvement-77-1.2842.hdf5\n",
      "Epoch 78/200\n",
      "26785/26785 [==============================] - 75s 3ms/step - loss: 1.2544\n",
      "\n",
      "Epoch 00078: loss improved from 1.28419 to 1.25437, saving model to weights-improvement-78-1.2544.hdf5\n",
      "Epoch 79/200\n",
      "26785/26785 [==============================] - 74s 3ms/step - loss: 1.2642\n",
      "\n",
      "Epoch 00079: loss did not improve from 1.25437\n",
      "Epoch 80/200\n",
      "26785/26785 [==============================] - 75s 3ms/step - loss: 1.2585\n",
      "\n",
      "Epoch 00080: loss did not improve from 1.25437\n",
      "Epoch 81/200\n",
      "26785/26785 [==============================] - 78s 3ms/step - loss: 1.2964\n",
      "\n",
      "Epoch 00081: loss did not improve from 1.25437\n",
      "Epoch 82/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26785/26785 [==============================] - 113s 4ms/step - loss: 1.2404\n",
      "\n",
      "Epoch 00082: loss improved from 1.25437 to 1.24042, saving model to weights-improvement-82-1.2404.hdf5\n",
      "Epoch 83/200\n",
      "26785/26785 [==============================] - 104s 4ms/step - loss: 1.2528\n",
      "\n",
      "Epoch 00083: loss did not improve from 1.24042\n",
      "Epoch 84/200\n",
      "26785/26785 [==============================] - 76s 3ms/step - loss: 1.2505\n",
      "\n",
      "Epoch 00084: loss did not improve from 1.24042\n",
      "Epoch 85/200\n",
      "26785/26785 [==============================] - 71s 3ms/step - loss: 1.2164\n",
      "\n",
      "Epoch 00085: loss improved from 1.24042 to 1.21638, saving model to weights-improvement-85-1.2164.hdf5\n",
      "Epoch 86/200\n",
      "26785/26785 [==============================] - 72s 3ms/step - loss: 1.2379\n",
      "\n",
      "Epoch 00086: loss did not improve from 1.21638\n",
      "Epoch 87/200\n",
      "26785/26785 [==============================] - 75s 3ms/step - loss: 1.1754\n",
      "\n",
      "Epoch 00087: loss improved from 1.21638 to 1.17538, saving model to weights-improvement-87-1.1754.hdf5\n",
      "Epoch 88/200\n",
      "26785/26785 [==============================] - 71s 3ms/step - loss: 1.1627\n",
      "\n",
      "Epoch 00088: loss improved from 1.17538 to 1.16270, saving model to weights-improvement-88-1.1627.hdf5\n",
      "Epoch 89/200\n",
      "26785/26785 [==============================] - 72s 3ms/step - loss: 1.1539\n",
      "\n",
      "Epoch 00089: loss improved from 1.16270 to 1.15390, saving model to weights-improvement-89-1.1539.hdf5\n",
      "Epoch 90/200\n",
      "26785/26785 [==============================] - 72s 3ms/step - loss: 1.1775\n",
      "\n",
      "Epoch 00090: loss did not improve from 1.15390\n",
      "Epoch 91/200\n",
      "26785/26785 [==============================] - 71s 3ms/step - loss: 1.2902\n",
      "\n",
      "Epoch 00091: loss did not improve from 1.15390\n",
      "Epoch 92/200\n",
      "26785/26785 [==============================] - 71s 3ms/step - loss: 1.2339\n",
      "\n",
      "Epoch 00092: loss did not improve from 1.15390\n",
      "Epoch 93/200\n",
      "26785/26785 [==============================] - 71s 3ms/step - loss: 1.1684\n",
      "\n",
      "Epoch 00093: loss did not improve from 1.15390\n",
      "Epoch 94/200\n",
      "26785/26785 [==============================] - 72s 3ms/step - loss: 1.1047\n",
      "\n",
      "Epoch 00094: loss improved from 1.15390 to 1.10470, saving model to weights-improvement-94-1.1047.hdf5\n",
      "Epoch 95/200\n",
      "26785/26785 [==============================] - 73s 3ms/step - loss: 1.1123\n",
      "\n",
      "Epoch 00095: loss did not improve from 1.10470\n",
      "Epoch 96/200\n",
      "26785/26785 [==============================] - 95s 4ms/step - loss: 1.1018\n",
      "\n",
      "Epoch 00096: loss improved from 1.10470 to 1.10179, saving model to weights-improvement-96-1.1018.hdf5\n",
      "Epoch 97/200\n",
      "26785/26785 [==============================] - 101s 4ms/step - loss: 1.0764\n",
      "\n",
      "Epoch 00097: loss improved from 1.10179 to 1.07639, saving model to weights-improvement-97-1.0764.hdf5\n",
      "Epoch 98/200\n",
      "26785/26785 [==============================] - 102s 4ms/step - loss: 1.0381\n",
      "\n",
      "Epoch 00098: loss improved from 1.07639 to 1.03810, saving model to weights-improvement-98-1.0381.hdf5\n",
      "Epoch 99/200\n",
      "26785/26785 [==============================] - 99s 4ms/step - loss: 1.1365\n",
      "\n",
      "Epoch 00099: loss did not improve from 1.03810\n",
      "Epoch 100/200\n",
      "26785/26785 [==============================] - 101s 4ms/step - loss: 1.0717\n",
      "\n",
      "Epoch 00100: loss did not improve from 1.03810\n",
      "Epoch 101/200\n",
      "26785/26785 [==============================] - 102s 4ms/step - loss: 1.0965\n",
      "\n",
      "Epoch 00101: loss did not improve from 1.03810\n",
      "Epoch 102/200\n",
      "26785/26785 [==============================] - 102s 4ms/step - loss: 1.1093\n",
      "\n",
      "Epoch 00102: loss did not improve from 1.03810\n",
      "Epoch 103/200\n",
      "26785/26785 [==============================] - 102s 4ms/step - loss: 1.0469\n",
      "\n",
      "Epoch 00103: loss did not improve from 1.03810\n",
      "Epoch 104/200\n",
      "26785/26785 [==============================] - 103s 4ms/step - loss: 1.0294\n",
      "\n",
      "Epoch 00104: loss improved from 1.03810 to 1.02943, saving model to weights-improvement-104-1.0294.hdf5\n",
      "Epoch 105/200\n",
      "26785/26785 [==============================] - 102s 4ms/step - loss: 0.9994\n",
      "\n",
      "Epoch 00105: loss improved from 1.02943 to 0.99941, saving model to weights-improvement-105-0.9994.hdf5\n",
      "Epoch 106/200\n",
      "26785/26785 [==============================] - 103s 4ms/step - loss: 0.9922\n",
      "\n",
      "Epoch 00106: loss improved from 0.99941 to 0.99220, saving model to weights-improvement-106-0.9922.hdf5\n",
      "Epoch 107/200\n",
      "26785/26785 [==============================] - 104s 4ms/step - loss: 2.3485\n",
      "\n",
      "Epoch 00107: loss did not improve from 0.99220\n",
      "Epoch 108/200\n",
      "26785/26785 [==============================] - 103s 4ms/step - loss: 2.2862\n",
      "\n",
      "Epoch 00108: loss did not improve from 0.99220\n",
      "Epoch 109/200\n",
      "26785/26785 [==============================] - 96s 4ms/step - loss: 1.5394\n",
      "\n",
      "Epoch 00109: loss did not improve from 0.99220\n",
      "Epoch 110/200\n",
      "26785/26785 [==============================] - 97s 4ms/step - loss: 1.2127\n",
      "\n",
      "Epoch 00110: loss did not improve from 0.99220\n",
      "Epoch 111/200\n",
      "26785/26785 [==============================] - 100s 4ms/step - loss: 1.0760\n",
      "\n",
      "Epoch 00111: loss did not improve from 0.99220\n",
      "Epoch 112/200\n",
      "26785/26785 [==============================] - 100s 4ms/step - loss: 1.0216\n",
      "\n",
      "Epoch 00112: loss did not improve from 0.99220\n",
      "Epoch 113/200\n",
      "26785/26785 [==============================] - 100s 4ms/step - loss: 0.9945\n",
      "\n",
      "Epoch 00113: loss did not improve from 0.99220\n",
      "Epoch 114/200\n",
      "26785/26785 [==============================] - 100s 4ms/step - loss: 1.0005\n",
      "\n",
      "Epoch 00114: loss did not improve from 0.99220\n",
      "Epoch 115/200\n",
      "26785/26785 [==============================] - 87s 3ms/step - loss: 1.0468\n",
      "\n",
      "Epoch 00115: loss did not improve from 0.99220\n",
      "Epoch 116/200\n",
      "26785/26785 [==============================] - 73s 3ms/step - loss: 1.0197\n",
      "\n",
      "Epoch 00116: loss did not improve from 0.99220\n",
      "Epoch 117/200\n",
      "26785/26785 [==============================] - 74s 3ms/step - loss: 0.9946\n",
      "\n",
      "Epoch 00117: loss did not improve from 0.99220\n",
      "Epoch 118/200\n",
      "26785/26785 [==============================] - 74s 3ms/step - loss: 0.9460\n",
      "\n",
      "Epoch 00118: loss improved from 0.99220 to 0.94604, saving model to weights-improvement-118-0.9460.hdf5\n",
      "Epoch 119/200\n",
      "26785/26785 [==============================] - 74s 3ms/step - loss: 1.0026\n",
      "\n",
      "Epoch 00119: loss did not improve from 0.94604\n",
      "Epoch 120/200\n",
      "26785/26785 [==============================] - 81s 3ms/step - loss: 0.9530\n",
      "\n",
      "Epoch 00120: loss did not improve from 0.94604\n",
      "Epoch 121/200\n",
      "26785/26785 [==============================] - 80s 3ms/step - loss: 0.9919\n",
      "\n",
      "Epoch 00121: loss did not improve from 0.94604\n",
      "Epoch 122/200\n",
      "26785/26785 [==============================] - 77s 3ms/step - loss: 1.0150\n",
      "\n",
      "Epoch 00122: loss did not improve from 0.94604\n",
      "Epoch 123/200\n",
      "26785/26785 [==============================] - 75s 3ms/step - loss: 0.9528\n",
      "\n",
      "Epoch 00123: loss did not improve from 0.94604\n",
      "Epoch 124/200\n",
      "26785/26785 [==============================] - 76s 3ms/step - loss: 1.2835\n",
      "\n",
      "Epoch 00124: loss did not improve from 0.94604\n",
      "Epoch 125/200\n",
      "26785/26785 [==============================] - 74s 3ms/step - loss: 2.7711\n",
      "\n",
      "Epoch 00125: loss did not improve from 0.94604\n",
      "Epoch 126/200\n",
      "26785/26785 [==============================] - 74s 3ms/step - loss: 1.4682\n",
      "\n",
      "Epoch 00126: loss did not improve from 0.94604\n",
      "Epoch 127/200\n",
      "26785/26785 [==============================] - 74s 3ms/step - loss: 1.0757\n",
      "\n",
      "Epoch 00127: loss did not improve from 0.94604\n",
      "Epoch 128/200\n",
      "26785/26785 [==============================] - 74s 3ms/step - loss: 0.9738\n",
      "\n",
      "Epoch 00128: loss did not improve from 0.94604\n",
      "Epoch 129/200\n",
      "26785/26785 [==============================] - 75s 3ms/step - loss: 0.9239\n",
      "\n",
      "Epoch 00129: loss improved from 0.94604 to 0.92385, saving model to weights-improvement-129-0.9239.hdf5\n",
      "Epoch 130/200\n",
      "26785/26785 [==============================] - 75s 3ms/step - loss: 0.9499\n",
      "\n",
      "Epoch 00130: loss did not improve from 0.92385\n",
      "Epoch 131/200\n",
      "26785/26785 [==============================] - 74s 3ms/step - loss: 0.9333\n",
      "\n",
      "Epoch 00131: loss did not improve from 0.92385\n",
      "Epoch 132/200\n",
      "26785/26785 [==============================] - 74s 3ms/step - loss: 0.9857\n",
      "\n",
      "Epoch 00132: loss did not improve from 0.92385\n",
      "Epoch 133/200\n",
      "26785/26785 [==============================] - 74s 3ms/step - loss: 0.9153\n",
      "\n",
      "Epoch 00133: loss improved from 0.92385 to 0.91529, saving model to weights-improvement-133-0.9153.hdf5\n",
      "Epoch 134/200\n",
      "26785/26785 [==============================] - 75s 3ms/step - loss: 0.8830\n",
      "\n",
      "Epoch 00134: loss improved from 0.91529 to 0.88301, saving model to weights-improvement-134-0.8830.hdf5\n",
      "Epoch 135/200\n",
      "26785/26785 [==============================] - 75s 3ms/step - loss: 0.8895\n",
      "\n",
      "Epoch 00135: loss did not improve from 0.88301\n",
      "Epoch 136/200\n",
      "26785/26785 [==============================] - 75s 3ms/step - loss: 0.8902\n",
      "\n",
      "Epoch 00136: loss did not improve from 0.88301\n",
      "Epoch 137/200\n",
      "26785/26785 [==============================] - 75s 3ms/step - loss: 0.9637\n",
      "\n",
      "Epoch 00137: loss did not improve from 0.88301\n",
      "Epoch 138/200\n",
      "26785/26785 [==============================] - 75s 3ms/step - loss: 0.8908\n",
      "\n",
      "Epoch 00138: loss did not improve from 0.88301\n",
      "Epoch 139/200\n",
      "26785/26785 [==============================] - 74s 3ms/step - loss: 0.8694\n",
      "\n",
      "Epoch 00139: loss improved from 0.88301 to 0.86941, saving model to weights-improvement-139-0.8694.hdf5\n",
      "Epoch 140/200\n",
      "26785/26785 [==============================] - 74s 3ms/step - loss: 0.8661\n",
      "\n",
      "Epoch 00140: loss improved from 0.86941 to 0.86610, saving model to weights-improvement-140-0.8661.hdf5\n",
      "Epoch 141/200\n",
      "26785/26785 [==============================] - 74s 3ms/step - loss: 0.8923\n",
      "\n",
      "Epoch 00141: loss did not improve from 0.86610\n",
      "Epoch 142/200\n",
      "26785/26785 [==============================] - 75s 3ms/step - loss: 0.9084\n",
      "\n",
      "Epoch 00142: loss did not improve from 0.86610\n",
      "Epoch 143/200\n",
      "26785/26785 [==============================] - 74s 3ms/step - loss: 0.9131\n",
      "\n",
      "Epoch 00143: loss did not improve from 0.86610\n",
      "Epoch 144/200\n",
      "26785/26785 [==============================] - 74s 3ms/step - loss: 0.9964\n",
      "\n",
      "Epoch 00144: loss did not improve from 0.86610\n",
      "Epoch 145/200\n",
      "26785/26785 [==============================] - 74s 3ms/step - loss: 0.9637\n",
      "\n",
      "Epoch 00145: loss did not improve from 0.86610\n",
      "Epoch 146/200\n",
      "26785/26785 [==============================] - 75s 3ms/step - loss: 2.0007\n",
      "\n",
      "Epoch 00146: loss did not improve from 0.86610\n",
      "Epoch 147/200\n",
      "26785/26785 [==============================] - 75s 3ms/step - loss: 2.4155\n",
      "\n",
      "Epoch 00147: loss did not improve from 0.86610\n",
      "Epoch 148/200\n",
      "26785/26785 [==============================] - 74s 3ms/step - loss: 1.5222\n",
      "\n",
      "Epoch 00148: loss did not improve from 0.86610\n",
      "Epoch 149/200\n",
      "26785/26785 [==============================] - 76s 3ms/step - loss: 1.1359\n",
      "\n",
      "Epoch 00149: loss did not improve from 0.86610\n",
      "Epoch 150/200\n",
      "26785/26785 [==============================] - 76s 3ms/step - loss: 0.9776\n",
      "\n",
      "Epoch 00150: loss did not improve from 0.86610\n",
      "Epoch 151/200\n",
      "26785/26785 [==============================] - 75s 3ms/step - loss: 0.8949\n",
      "\n",
      "Epoch 00151: loss did not improve from 0.86610\n",
      "Epoch 152/200\n",
      "26785/26785 [==============================] - 74s 3ms/step - loss: 0.8591\n",
      "\n",
      "Epoch 00152: loss improved from 0.86610 to 0.85908, saving model to weights-improvement-152-0.8591.hdf5\n",
      "Epoch 153/200\n",
      "26785/26785 [==============================] - 74s 3ms/step - loss: 0.8430\n",
      "\n",
      "Epoch 00153: loss improved from 0.85908 to 0.84301, saving model to weights-improvement-153-0.8430.hdf5\n",
      "Epoch 154/200\n",
      "26785/26785 [==============================] - 77s 3ms/step - loss: 0.9107\n",
      "\n",
      "Epoch 00154: loss did not improve from 0.84301\n",
      "Epoch 155/200\n",
      "26785/26785 [==============================] - 77s 3ms/step - loss: 0.8145\n",
      "\n",
      "Epoch 00155: loss improved from 0.84301 to 0.81455, saving model to weights-improvement-155-0.8145.hdf5\n",
      "Epoch 156/200\n",
      "26785/26785 [==============================] - 75s 3ms/step - loss: 0.8102\n",
      "\n",
      "Epoch 00156: loss improved from 0.81455 to 0.81024, saving model to weights-improvement-156-0.8102.hdf5\n",
      "Epoch 157/200\n",
      "26785/26785 [==============================] - 77s 3ms/step - loss: 0.8121\n",
      "\n",
      "Epoch 00157: loss did not improve from 0.81024\n",
      "Epoch 158/200\n",
      "26785/26785 [==============================] - 75s 3ms/step - loss: 0.8036\n",
      "\n",
      "Epoch 00158: loss improved from 0.81024 to 0.80361, saving model to weights-improvement-158-0.8036.hdf5\n",
      "Epoch 159/200\n",
      "26785/26785 [==============================] - 77s 3ms/step - loss: 0.8502\n",
      "\n",
      "Epoch 00159: loss did not improve from 0.80361\n",
      "Epoch 160/200\n",
      "26785/26785 [==============================] - 75s 3ms/step - loss: 0.9414\n",
      "\n",
      "Epoch 00160: loss did not improve from 0.80361\n",
      "Epoch 161/200\n",
      "26785/26785 [==============================] - 73s 3ms/step - loss: 0.9607\n",
      "\n",
      "Epoch 00161: loss did not improve from 0.80361\n",
      "Epoch 162/200\n",
      "26785/26785 [==============================] - 74s 3ms/step - loss: 0.8645\n",
      "\n",
      "Epoch 00162: loss did not improve from 0.80361\n",
      "Epoch 163/200\n",
      "26785/26785 [==============================] - 74s 3ms/step - loss: 0.8358\n",
      "\n",
      "Epoch 00163: loss did not improve from 0.80361\n",
      "Epoch 164/200\n",
      "26785/26785 [==============================] - 77s 3ms/step - loss: 0.7867\n",
      "\n",
      "Epoch 00164: loss improved from 0.80361 to 0.78671, saving model to weights-improvement-164-0.7867.hdf5\n",
      "Epoch 165/200\n",
      "26785/26785 [==============================] - 76s 3ms/step - loss: 0.7710\n",
      "\n",
      "Epoch 00165: loss improved from 0.78671 to 0.77104, saving model to weights-improvement-165-0.7710.hdf5\n",
      "Epoch 166/200\n",
      "26785/26785 [==============================] - 74s 3ms/step - loss: 2.7027\n",
      "\n",
      "Epoch 00166: loss did not improve from 0.77104\n",
      "Epoch 167/200\n",
      "26785/26785 [==============================] - 74s 3ms/step - loss: 2.1519\n",
      "\n",
      "Epoch 00167: loss did not improve from 0.77104\n",
      "Epoch 168/200\n",
      "26785/26785 [==============================] - 74s 3ms/step - loss: 1.3332\n",
      "\n",
      "Epoch 00168: loss did not improve from 0.77104\n",
      "Epoch 169/200\n",
      "26785/26785 [==============================] - 73s 3ms/step - loss: 1.0202\n",
      "\n",
      "Epoch 00169: loss did not improve from 0.77104\n",
      "Epoch 170/200\n",
      "26785/26785 [==============================] - 73s 3ms/step - loss: 0.8799\n",
      "\n",
      "Epoch 00170: loss did not improve from 0.77104\n",
      "Epoch 171/200\n",
      "26785/26785 [==============================] - 74s 3ms/step - loss: 0.8242\n",
      "\n",
      "Epoch 00171: loss did not improve from 0.77104\n",
      "Epoch 172/200\n",
      "26785/26785 [==============================] - 78s 3ms/step - loss: 0.8074\n",
      "\n",
      "Epoch 00172: loss did not improve from 0.77104\n",
      "Epoch 173/200\n",
      "26785/26785 [==============================] - 84s 3ms/step - loss: 0.8056\n",
      "\n",
      "Epoch 00173: loss did not improve from 0.77104\n",
      "Epoch 174/200\n",
      "26785/26785 [==============================] - 75s 3ms/step - loss: 0.7965\n",
      "\n",
      "Epoch 00174: loss did not improve from 0.77104\n",
      "Epoch 175/200\n",
      "26785/26785 [==============================] - 75s 3ms/step - loss: 0.9190\n",
      "\n",
      "Epoch 00175: loss did not improve from 0.77104\n",
      "Epoch 176/200\n",
      "26785/26785 [==============================] - 75s 3ms/step - loss: 0.9788\n",
      "\n",
      "Epoch 00176: loss did not improve from 0.77104\n",
      "Epoch 177/200\n",
      "26785/26785 [==============================] - 77s 3ms/step - loss: 0.8448\n",
      "\n",
      "Epoch 00177: loss did not improve from 0.77104\n",
      "Epoch 178/200\n",
      "26785/26785 [==============================] - 80s 3ms/step - loss: 0.8381\n",
      "\n",
      "Epoch 00178: loss did not improve from 0.77104\n",
      "Epoch 179/200\n",
      "26785/26785 [==============================] - 76s 3ms/step - loss: 0.7939\n",
      "\n",
      "Epoch 00179: loss did not improve from 0.77104\n",
      "Epoch 180/200\n",
      "26785/26785 [==============================] - 74s 3ms/step - loss: 0.7763\n",
      "\n",
      "Epoch 00180: loss did not improve from 0.77104\n",
      "Epoch 181/200\n",
      "26785/26785 [==============================] - 74s 3ms/step - loss: 0.8431\n",
      "\n",
      "Epoch 00181: loss did not improve from 0.77104\n",
      "Epoch 182/200\n",
      "26785/26785 [==============================] - 74s 3ms/step - loss: 0.7682\n",
      "\n",
      "Epoch 00182: loss improved from 0.77104 to 0.76817, saving model to weights-improvement-182-0.7682.hdf5\n",
      "Epoch 183/200\n",
      "26785/26785 [==============================] - 74s 3ms/step - loss: 0.7938\n",
      "\n",
      "Epoch 00183: loss did not improve from 0.76817\n",
      "Epoch 184/200\n",
      "26785/26785 [==============================] - 74s 3ms/step - loss: 0.8107\n",
      "\n",
      "Epoch 00184: loss did not improve from 0.76817\n",
      "Epoch 185/200\n",
      "26785/26785 [==============================] - 73s 3ms/step - loss: 2.5613\n",
      "\n",
      "Epoch 00185: loss did not improve from 0.76817\n",
      "Epoch 186/200\n",
      "26785/26785 [==============================] - 74s 3ms/step - loss: 3.1890\n",
      "\n",
      "Epoch 00186: loss did not improve from 0.76817\n",
      "Epoch 187/200\n",
      "26785/26785 [==============================] - 77s 3ms/step - loss: 2.9067\n",
      "\n",
      "Epoch 00187: loss did not improve from 0.76817\n",
      "Epoch 188/200\n",
      "26785/26785 [==============================] - 75s 3ms/step - loss: 2.8143\n",
      "\n",
      "Epoch 00188: loss did not improve from 0.76817\n",
      "Epoch 189/200\n",
      "26785/26785 [==============================] - 81s 3ms/step - loss: 2.7717\n",
      "\n",
      "Epoch 00189: loss did not improve from 0.76817\n",
      "Epoch 190/200\n",
      "26785/26785 [==============================] - 77s 3ms/step - loss: 2.7328\n",
      "\n",
      "Epoch 00190: loss did not improve from 0.76817\n",
      "Epoch 191/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26785/26785 [==============================] - 75s 3ms/step - loss: 2.7102\n",
      "\n",
      "Epoch 00191: loss did not improve from 0.76817\n",
      "Epoch 192/200\n",
      "26785/26785 [==============================] - 76s 3ms/step - loss: 2.6849\n",
      "\n",
      "Epoch 00192: loss did not improve from 0.76817\n",
      "Epoch 193/200\n",
      "26785/26785 [==============================] - 74s 3ms/step - loss: 2.6658\n",
      "\n",
      "Epoch 00193: loss did not improve from 0.76817\n",
      "Epoch 194/200\n",
      "26785/26785 [==============================] - 73s 3ms/step - loss: 2.6394\n",
      "\n",
      "Epoch 00194: loss did not improve from 0.76817\n",
      "Epoch 195/200\n",
      "26785/26785 [==============================] - 74s 3ms/step - loss: 2.6183\n",
      "\n",
      "Epoch 00195: loss did not improve from 0.76817\n",
      "Epoch 196/200\n",
      "26785/26785 [==============================] - 73s 3ms/step - loss: 2.6005\n",
      "\n",
      "Epoch 00196: loss did not improve from 0.76817\n",
      "Epoch 197/200\n",
      "26785/26785 [==============================] - 73s 3ms/step - loss: 2.5729\n",
      "\n",
      "Epoch 00197: loss did not improve from 0.76817\n",
      "Epoch 198/200\n",
      "26785/26785 [==============================] - 73s 3ms/step - loss: 2.5485\n",
      "\n",
      "Epoch 00198: loss did not improve from 0.76817\n",
      "Epoch 199/200\n",
      "26785/26785 [==============================] - 73s 3ms/step - loss: 2.5184\n",
      "\n",
      "Epoch 00199: loss did not improve from 0.76817\n",
      "Epoch 200/200\n",
      "26785/26785 [==============================] - 73s 3ms/step - loss: 2.4876\n",
      "\n",
      "Epoch 00200: loss did not improve from 0.76817\n",
      "Seed #1:\n",
      "\" him could have died. If he had his choice, Death knew Reckless would have preferred to die on one of \"\n",
      "Seed #2:\n",
      "\" kly caked on her face, and her hair was a greasy shade of blonde that could have only come from a bo \"\n",
      "Seed #3:\n",
      "\" ght was clear and cold. Above him, stars sparkled in the dark blue sky, the pale moon shining. Reckl \"\n",
      "Seed #4:\n",
      "\" ises,\" I said.   This brought me to where I am. I didn't have any current cases, so as soon as I lef \"\n",
      "Seed #5:\n",
      "\"  the spent butt of his cigarette from his thin lips, cracked open his window, and flicked it out ont \"\n",
      "Seed #6:\n",
      "\" mportant. Besides, does it matter? Look at these people. They're happy I found it.\" Gustaf surveyed  \"\n",
      "Seed #7:\n",
      "\" long day at work. He'd smile, jokingly comment on how strongly we smelled from playing outside, then \"\n",
      "Seed #8:\n",
      "\" rn home one month later.  At least, that was the plan. Before Clarke started complaining of chest pa \"\n",
      "Seed #9:\n",
      "\" ocks, the stars, and the Earth for company, he would wait.  I stared down at the card in my lap. On  \"\n",
      "Seed #10:\n",
      "\" t the bar. He motioned to the barmaid, the maiden from earlier, and she brought him another drink wi \"\n",
      " his qu ths fongt. the lrsse of th  rrl fomais st shnheng  wot lnle fo the cossend  \"ehe dorver,\" qe\n",
      "ttle. Clotnh was so tol mouger ti th il bi khei to ie the moice an a smil himert ar the aroene.  Hee\n",
      "ess grinded, shan haid uo bate ooer thah to hard ane souser th taa lumin ae anrma eereense.te the ro\n",
      "t the sronery store  Ie eedny then te te the woilt ware at a sichti.a lpill anact worldne aod haltre\n",
      "o the corises. The thas man in the basksea  lheng was on the pant atoonther wo the het  He wos clea \n",
      "the drowd, which was on yhe frist cros of tis corkeer. I smi oo mim moull be kn dned hes rorhent th \n",
      " herd us back into the dorme to wast op fir dimine. \"Hor hid mee th th, shs mfn the th sotern ar?\" R\n",
      "ins as the sereowa alaren sikt goanhns, when he wotad eis seatost of the cuo se the momsh op thesen \n",
      "the card wn nene of was demleeo. Tee womst ss thrn a ceulld berer alan   boane sould  toile    \" Soc\n",
      "th a smiler cilares.  Teat s soen,it h pame,toaeth an   Iimanes leml lee soert dak arass arr cems to\n"
     ]
    }
   ],
   "source": [
    "rstories_texts = write_weights(rstories_fulltext,100,200,10,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "hhg_fulltext = ''\n",
    "f = 'hhg.txt'\n",
    "file = open(f)\n",
    "hhg_fulltext = file.read()\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "hhg_fulltext = hhg_fulltext.replace('THE HITCHHIKER\\'S GUIDE TO THE GALAXY\\nBY DOUGLAS ADAMS\\n','').replace('\\n',' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Characters:  266523\n",
      "Total Vocab (Unique Characters):  86\n",
      "Total Patterns:  266513 \n",
      "\n",
      "Epoch 1/200\n",
      "266513/266513 [==============================] - 96s 360us/step - loss: 3.0048\n",
      "\n",
      "Epoch 00001: loss improved from inf to 3.00484, saving model to weights-improvement-01-3.0048.hdf5\n",
      "Epoch 2/200\n",
      "266513/266513 [==============================] - 86s 322us/step - loss: 2.8487\n",
      "\n",
      "Epoch 00002: loss improved from 3.00484 to 2.84868, saving model to weights-improvement-02-2.8487.hdf5\n",
      "Epoch 3/200\n",
      "266513/266513 [==============================] - 81s 303us/step - loss: 2.7818\n",
      "\n",
      "Epoch 00003: loss improved from 2.84868 to 2.78184, saving model to weights-improvement-03-2.7818.hdf5\n",
      "Epoch 4/200\n",
      "266513/266513 [==============================] - 81s 304us/step - loss: 2.7337\n",
      "\n",
      "Epoch 00004: loss improved from 2.78184 to 2.73365, saving model to weights-improvement-04-2.7337.hdf5\n",
      "Epoch 5/200\n",
      "266513/266513 [==============================] - 82s 307us/step - loss: 2.6887\n",
      "\n",
      "Epoch 00005: loss improved from 2.73365 to 2.68870, saving model to weights-improvement-05-2.6887.hdf5\n",
      "Epoch 6/200\n",
      "266513/266513 [==============================] - 82s 306us/step - loss: 2.6472\n",
      "\n",
      "Epoch 00006: loss improved from 2.68870 to 2.64723, saving model to weights-improvement-06-2.6472.hdf5\n",
      "Epoch 7/200\n",
      "266513/266513 [==============================] - 82s 307us/step - loss: 2.6086\n",
      "\n",
      "Epoch 00007: loss improved from 2.64723 to 2.60862, saving model to weights-improvement-07-2.6086.hdf5\n",
      "Epoch 8/200\n",
      "266513/266513 [==============================] - 82s 307us/step - loss: 2.5727\n",
      "\n",
      "Epoch 00008: loss improved from 2.60862 to 2.57275, saving model to weights-improvement-08-2.5727.hdf5\n",
      "Epoch 9/200\n",
      "266513/266513 [==============================] - 82s 309us/step - loss: 2.5397\n",
      "\n",
      "Epoch 00009: loss improved from 2.57275 to 2.53966, saving model to weights-improvement-09-2.5397.hdf5\n",
      "Epoch 10/200\n",
      "266513/266513 [==============================] - 82s 309us/step - loss: 2.5097\n",
      "\n",
      "Epoch 00010: loss improved from 2.53966 to 2.50971, saving model to weights-improvement-10-2.5097.hdf5\n",
      "Epoch 11/200\n",
      "266513/266513 [==============================] - 82s 308us/step - loss: 2.4811\n",
      "\n",
      "Epoch 00011: loss improved from 2.50971 to 2.48115, saving model to weights-improvement-11-2.4811.hdf5\n",
      "Epoch 12/200\n",
      "266513/266513 [==============================] - 82s 309us/step - loss: 2.4541\n",
      "\n",
      "Epoch 00012: loss improved from 2.48115 to 2.45414, saving model to weights-improvement-12-2.4541.hdf5\n",
      "Epoch 13/200\n",
      "266513/266513 [==============================] - 86s 324us/step - loss: 2.4294\n",
      "\n",
      "Epoch 00013: loss improved from 2.45414 to 2.42938, saving model to weights-improvement-13-2.4294.hdf5\n",
      "Epoch 14/200\n",
      "266513/266513 [==============================] - 89s 333us/step - loss: 2.4028\n",
      "\n",
      "Epoch 00014: loss improved from 2.42938 to 2.40278, saving model to weights-improvement-14-2.4028.hdf5\n",
      "Epoch 15/200\n",
      "266513/266513 [==============================] - 90s 337us/step - loss: 2.38022s  - ETA\n",
      "\n",
      "Epoch 00015: loss improved from 2.40278 to 2.38024, saving model to weights-improvement-15-2.3802.hdf5\n",
      "Epoch 16/200\n",
      "266513/266513 [==============================] - 90s 336us/step - loss: 2.3542\n",
      "\n",
      "Epoch 00016: loss improved from 2.38024 to 2.35418, saving model to weights-improvement-16-2.3542.hdf5\n",
      "Epoch 17/200\n",
      "266513/266513 [==============================] - 90s 337us/step - loss: 2.3295\n",
      "\n",
      "Epoch 00017: loss improved from 2.35418 to 2.32954, saving model to weights-improvement-17-2.3295.hdf5\n",
      "Epoch 18/200\n",
      "266513/266513 [==============================] - 89s 335us/step - loss: 2.3081\n",
      "\n",
      "Epoch 00018: loss improved from 2.32954 to 2.30806, saving model to weights-improvement-18-2.3081.hdf5\n",
      "Epoch 19/200\n",
      "266513/266513 [==============================] - 92s 346us/step - loss: 2.2865\n",
      "\n",
      "Epoch 00019: loss improved from 2.30806 to 2.28654, saving model to weights-improvement-19-2.2865.hdf5\n",
      "Epoch 20/200\n",
      "266513/266513 [==============================] - 93s 348us/step - loss: 2.2681\n",
      "\n",
      "Epoch 00020: loss improved from 2.28654 to 2.26807, saving model to weights-improvement-20-2.2681.hdf5\n",
      "Epoch 21/200\n",
      "266513/266513 [==============================] - 93s 348us/step - loss: 2.2486\n",
      "\n",
      "Epoch 00021: loss improved from 2.26807 to 2.24860, saving model to weights-improvement-21-2.2486.hdf5\n",
      "Epoch 22/200\n",
      "266513/266513 [==============================] - 93s 350us/step - loss: 2.2312\n",
      "\n",
      "Epoch 00022: loss improved from 2.24860 to 2.23119, saving model to weights-improvement-22-2.2312.hdf5\n",
      "Epoch 23/200\n",
      "266513/266513 [==============================] - 94s 351us/step - loss: 2.2151\n",
      "\n",
      "Epoch 00023: loss improved from 2.23119 to 2.21507, saving model to weights-improvement-23-2.2151.hdf5\n",
      "Epoch 24/200\n",
      "266513/266513 [==============================] - 94s 353us/step - loss: 2.2004\n",
      "\n",
      "Epoch 00024: loss improved from 2.21507 to 2.20042, saving model to weights-improvement-24-2.2004.hdf5\n",
      "Epoch 25/200\n",
      "266513/266513 [==============================] - 95s 357us/step - loss: 2.1866\n",
      "\n",
      "Epoch 00025: loss improved from 2.20042 to 2.18663, saving model to weights-improvement-25-2.1866.hdf5\n",
      "Epoch 26/200\n",
      "266513/266513 [==============================] - 95s 358us/step - loss: 2.1727\n",
      "\n",
      "Epoch 00026: loss improved from 2.18663 to 2.17267, saving model to weights-improvement-26-2.1727.hdf5\n",
      "Epoch 27/200\n",
      "266513/266513 [==============================] - 95s 358us/step - loss: 2.1594\n",
      "\n",
      "Epoch 00027: loss improved from 2.17267 to 2.15944, saving model to weights-improvement-27-2.1594.hdf5\n",
      "Epoch 28/200\n",
      "266513/266513 [==============================] - 96s 360us/step - loss: 2.1458\n",
      "\n",
      "Epoch 00028: loss improved from 2.15944 to 2.14581, saving model to weights-improvement-28-2.1458.hdf5\n",
      "Epoch 29/200\n",
      "266513/266513 [==============================] - 96s 360us/step - loss: 2.1343\n",
      "\n",
      "Epoch 00029: loss improved from 2.14581 to 2.13433, saving model to weights-improvement-29-2.1343.hdf5\n",
      "Epoch 30/200\n",
      "266513/266513 [==============================] - 99s 371us/step - loss: 2.1210\n",
      "\n",
      "Epoch 00030: loss improved from 2.13433 to 2.12100, saving model to weights-improvement-30-2.1210.hdf5\n",
      "Epoch 31/200\n",
      "266513/266513 [==============================] - 96s 359us/step - loss: 2.1102\n",
      "\n",
      "Epoch 00031: loss improved from 2.12100 to 2.11019, saving model to weights-improvement-31-2.1102.hdf5\n",
      "Epoch 32/200\n",
      "266513/266513 [==============================] - 95s 355us/step - loss: 2.1008\n",
      "\n",
      "Epoch 00032: loss improved from 2.11019 to 2.10081, saving model to weights-improvement-32-2.1008.hdf5\n",
      "Epoch 33/200\n",
      "266513/266513 [==============================] - 94s 353us/step - loss: 2.0867\n",
      "\n",
      "Epoch 00033: loss improved from 2.10081 to 2.08674, saving model to weights-improvement-33-2.0867.hdf5\n",
      "Epoch 34/200\n",
      "266513/266513 [==============================] - 95s 357us/step - loss: 2.0780\n",
      "\n",
      "Epoch 00034: loss improved from 2.08674 to 2.07797, saving model to weights-improvement-34-2.0780.hdf5\n",
      "Epoch 35/200\n",
      "266513/266513 [==============================] - 94s 352us/step - loss: 2.0696\n",
      "\n",
      "Epoch 00035: loss improved from 2.07797 to 2.06962, saving model to weights-improvement-35-2.0696.hdf5\n",
      "Epoch 36/200\n",
      "266513/266513 [==============================] - 96s 360us/step - loss: 2.0596\n",
      "\n",
      "Epoch 00036: loss improved from 2.06962 to 2.05960, saving model to weights-improvement-36-2.0596.hdf5\n",
      "Epoch 37/200\n",
      "266513/266513 [==============================] - 93s 349us/step - loss: 2.0488\n",
      "\n",
      "Epoch 00037: loss improved from 2.05960 to 2.04883, saving model to weights-improvement-37-2.0488.hdf5\n",
      "Epoch 38/200\n",
      "266513/266513 [==============================] - 93s 349us/step - loss: 2.0399\n",
      "\n",
      "Epoch 00038: loss improved from 2.04883 to 2.03991, saving model to weights-improvement-38-2.0399.hdf5\n",
      "Epoch 39/200\n",
      "266513/266513 [==============================] - 94s 354us/step - loss: 2.0331\n",
      "\n",
      "Epoch 00039: loss improved from 2.03991 to 2.03308, saving model to weights-improvement-39-2.0331.hdf5\n",
      "Epoch 40/200\n",
      "266513/266513 [==============================] - 92s 347us/step - loss: 2.0254\n",
      "\n",
      "Epoch 00040: loss improved from 2.03308 to 2.02544, saving model to weights-improvement-40-2.0254.hdf5\n",
      "Epoch 41/200\n",
      "266513/266513 [==============================] - 94s 353us/step - loss: 2.0141\n",
      "\n",
      "Epoch 00041: loss improved from 2.02544 to 2.01409, saving model to weights-improvement-41-2.0141.hdf5\n",
      "Epoch 42/200\n",
      "266513/266513 [==============================] - 96s 360us/step - loss: 2.0064\n",
      "\n",
      "Epoch 00042: loss improved from 2.01409 to 2.00636, saving model to weights-improvement-42-2.0064.hdf5\n",
      "Epoch 43/200\n",
      "266513/266513 [==============================] - 95s 356us/step - loss: 1.9976\n",
      "\n",
      "Epoch 00043: loss improved from 2.00636 to 1.99756, saving model to weights-improvement-43-1.9976.hdf5\n",
      "Epoch 44/200\n",
      "266513/266513 [==============================] - 94s 353us/step - loss: 1.9886\n",
      "\n",
      "Epoch 00044: loss improved from 1.99756 to 1.98860, saving model to weights-improvement-44-1.9886.hdf5\n",
      "Epoch 45/200\n",
      "266513/266513 [==============================] - 92s 347us/step - loss: 1.9833\n",
      "\n",
      "Epoch 00045: loss improved from 1.98860 to 1.98334, saving model to weights-improvement-45-1.9833.hdf5\n",
      "Epoch 46/200\n",
      "266513/266513 [==============================] - 90s 338us/step - loss: 1.97750s - lo\n",
      "\n",
      "Epoch 00046: loss improved from 1.98334 to 1.97748, saving model to weights-improvement-46-1.9775.hdf5\n",
      "Epoch 47/200\n",
      "266513/266513 [==============================] - 87s 325us/step - loss: 1.9688\n",
      "\n",
      "Epoch 00047: loss improved from 1.97748 to 1.96881, saving model to weights-improvement-47-1.9688.hdf5\n",
      "Epoch 48/200\n",
      "266513/266513 [==============================] - 84s 315us/step - loss: 1.9611\n",
      "\n",
      "Epoch 00048: loss improved from 1.96881 to 1.96110, saving model to weights-improvement-48-1.9611.hdf5\n",
      "Epoch 49/200\n",
      "266513/266513 [==============================] - 84s 316us/step - loss: 1.9581\n",
      "\n",
      "Epoch 00049: loss improved from 1.96110 to 1.95807, saving model to weights-improvement-49-1.9581.hdf5\n",
      "Epoch 50/200\n",
      "266513/266513 [==============================] - 85s 318us/step - loss: 1.9481\n",
      "\n",
      "Epoch 00050: loss improved from 1.95807 to 1.94812, saving model to weights-improvement-50-1.9481.hdf5\n",
      "Epoch 51/200\n",
      "266513/266513 [==============================] - 85s 318us/step - loss: 1.9424\n",
      "\n",
      "Epoch 00051: loss improved from 1.94812 to 1.94241, saving model to weights-improvement-51-1.9424.hdf5\n",
      "Epoch 52/200\n",
      "266513/266513 [==============================] - 84s 314us/step - loss: 1.9359\n",
      "\n",
      "Epoch 00052: loss improved from 1.94241 to 1.93595, saving model to weights-improvement-52-1.9359.hdf5\n",
      "Epoch 53/200\n",
      "266513/266513 [==============================] - 84s 314us/step - loss: 1.9295\n",
      "\n",
      "Epoch 00053: loss improved from 1.93595 to 1.92952, saving model to weights-improvement-53-1.9295.hdf5\n",
      "Epoch 54/200\n",
      "266513/266513 [==============================] - 84s 315us/step - loss: 1.9237\n",
      "\n",
      "Epoch 00054: loss improved from 1.92952 to 1.92365, saving model to weights-improvement-54-1.9237.hdf5\n",
      "Epoch 55/200\n",
      "266513/266513 [==============================] - 84s 316us/step - loss: 1.9167\n",
      "\n",
      "Epoch 00055: loss improved from 1.92365 to 1.91672, saving model to weights-improvement-55-1.9167.hdf5\n",
      "Epoch 56/200\n",
      "266513/266513 [==============================] - 84s 315us/step - loss: 1.9113\n",
      "\n",
      "Epoch 00056: loss improved from 1.91672 to 1.91135, saving model to weights-improvement-56-1.9113.hdf5\n",
      "Epoch 57/200\n",
      "266513/266513 [==============================] - 84s 316us/step - loss: 1.9047\n",
      "\n",
      "Epoch 00057: loss improved from 1.91135 to 1.90471, saving model to weights-improvement-57-1.9047.hdf5\n",
      "Epoch 58/200\n",
      "266513/266513 [==============================] - 90s 338us/step - loss: 1.8999\n",
      "\n",
      "Epoch 00058: loss improved from 1.90471 to 1.89991, saving model to weights-improvement-58-1.8999.hdf5\n",
      "Epoch 59/200\n",
      "266513/266513 [==============================] - 91s 340us/step - loss: 1.8955\n",
      "\n",
      "Epoch 00059: loss improved from 1.89991 to 1.89554, saving model to weights-improvement-59-1.8955.hdf5\n",
      "Epoch 60/200\n",
      "266513/266513 [==============================] - 90s 338us/step - loss: 1.8889\n",
      "\n",
      "Epoch 00060: loss improved from 1.89554 to 1.88891, saving model to weights-improvement-60-1.8889.hdf5\n",
      "Epoch 61/200\n",
      "266513/266513 [==============================] - 90s 338us/step - loss: 1.8864\n",
      "\n",
      "Epoch 00061: loss improved from 1.88891 to 1.88642, saving model to weights-improvement-61-1.8864.hdf5\n",
      "Epoch 62/200\n",
      "266513/266513 [==============================] - 91s 342us/step - loss: 1.8795\n",
      "\n",
      "Epoch 00062: loss improved from 1.88642 to 1.87955, saving model to weights-improvement-62-1.8795.hdf5\n",
      "Epoch 63/200\n",
      "266513/266513 [==============================] - 90s 338us/step - loss: 1.8739\n",
      "\n",
      "Epoch 00063: loss improved from 1.87955 to 1.87387, saving model to weights-improvement-63-1.8739.hdf5\n",
      "Epoch 64/200\n",
      "266513/266513 [==============================] - 90s 339us/step - loss: 1.8699\n",
      "\n",
      "Epoch 00064: loss improved from 1.87387 to 1.86990, saving model to weights-improvement-64-1.8699.hdf5\n",
      "Epoch 65/200\n",
      "266513/266513 [==============================] - 91s 343us/step - loss: 1.8666\n",
      "\n",
      "Epoch 00065: loss improved from 1.86990 to 1.86658, saving model to weights-improvement-65-1.8666.hdf5\n",
      "Epoch 66/200\n",
      "266513/266513 [==============================] - 91s 341us/step - loss: 1.8603\n",
      "\n",
      "Epoch 00066: loss improved from 1.86658 to 1.86033, saving model to weights-improvement-66-1.8603.hdf5\n",
      "Epoch 67/200\n",
      "266513/266513 [==============================] - 90s 339us/step - loss: 1.8555\n",
      "\n",
      "Epoch 00067: loss improved from 1.86033 to 1.85553, saving model to weights-improvement-67-1.8555.hdf5\n",
      "Epoch 68/200\n",
      "266513/266513 [==============================] - 91s 340us/step - loss: 1.8489\n",
      "\n",
      "Epoch 00068: loss improved from 1.85553 to 1.84889, saving model to weights-improvement-68-1.8489.hdf5\n",
      "Epoch 69/200\n",
      "266513/266513 [==============================] - 91s 341us/step - loss: 1.8461\n",
      "\n",
      "Epoch 00069: loss improved from 1.84889 to 1.84614, saving model to weights-improvement-69-1.8461.hdf5\n",
      "Epoch 70/200\n",
      "266513/266513 [==============================] - 91s 340us/step - loss: 1.8444\n",
      "\n",
      "Epoch 00070: loss improved from 1.84614 to 1.84441, saving model to weights-improvement-70-1.8444.hdf5\n",
      "Epoch 71/200\n",
      "266513/266513 [==============================] - 91s 341us/step - loss: 1.8402\n",
      "\n",
      "Epoch 00071: loss improved from 1.84441 to 1.84024, saving model to weights-improvement-71-1.8402.hdf5\n",
      "Epoch 72/200\n",
      "266513/266513 [==============================] - 91s 340us/step - loss: 1.83381s - lo - ETA: 0s - loss: 1\n",
      "\n",
      "Epoch 00072: loss improved from 1.84024 to 1.83376, saving model to weights-improvement-72-1.8338.hdf5\n",
      "Epoch 73/200\n",
      "266513/266513 [==============================] - 90s 338us/step - loss: 1.8307\n",
      "\n",
      "Epoch 00073: loss improved from 1.83376 to 1.83071, saving model to weights-improvement-73-1.8307.hdf5\n",
      "Epoch 74/200\n",
      "266513/266513 [==============================] - 93s 350us/step - loss: 1.8241\n",
      "\n",
      "Epoch 00074: loss improved from 1.83071 to 1.82413, saving model to weights-improvement-74-1.8241.hdf5\n",
      "Epoch 75/200\n",
      "266513/266513 [==============================] - 94s 353us/step - loss: 1.8214\n",
      "\n",
      "Epoch 00075: loss improved from 1.82413 to 1.82143, saving model to weights-improvement-75-1.8214.hdf5\n",
      "Epoch 76/200\n",
      "266513/266513 [==============================] - 91s 341us/step - loss: 1.8179\n",
      "\n",
      "Epoch 00076: loss improved from 1.82143 to 1.81791, saving model to weights-improvement-76-1.8179.hdf5\n",
      "Epoch 77/200\n",
      "266513/266513 [==============================] - 90s 339us/step - loss: 1.8139\n",
      "\n",
      "Epoch 00077: loss improved from 1.81791 to 1.81394, saving model to weights-improvement-77-1.8139.hdf5\n",
      "Epoch 78/200\n",
      "266513/266513 [==============================] - 92s 343us/step - loss: 1.8130\n",
      "\n",
      "Epoch 00078: loss improved from 1.81394 to 1.81298, saving model to weights-improvement-78-1.8130.hdf5\n",
      "Epoch 79/200\n",
      "266513/266513 [==============================] - 92s 344us/step - loss: 1.8055\n",
      "\n",
      "Epoch 00079: loss improved from 1.81298 to 1.80555, saving model to weights-improvement-79-1.8055.hdf5\n",
      "Epoch 80/200\n",
      "266513/266513 [==============================] - 92s 345us/step - loss: 1.8011\n",
      "\n",
      "Epoch 00080: loss improved from 1.80555 to 1.80110, saving model to weights-improvement-80-1.8011.hdf5\n",
      "Epoch 81/200\n",
      "266513/266513 [==============================] - 89s 332us/step - loss: 1.7988\n",
      "\n",
      "Epoch 00081: loss improved from 1.80110 to 1.79880, saving model to weights-improvement-81-1.7988.hdf5\n",
      "Epoch 82/200\n",
      "266513/266513 [==============================] - 83s 311us/step - loss: 1.7961\n",
      "\n",
      "Epoch 00082: loss improved from 1.79880 to 1.79613, saving model to weights-improvement-82-1.7961.hdf5\n",
      "Epoch 83/200\n",
      "266513/266513 [==============================] - 82s 309us/step - loss: 1.7912\n",
      "\n",
      "Epoch 00083: loss improved from 1.79613 to 1.79119, saving model to weights-improvement-83-1.7912.hdf5\n",
      "Epoch 84/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "266513/266513 [==============================] - 82s 308us/step - loss: 1.7848\n",
      "\n",
      "Epoch 00084: loss improved from 1.79119 to 1.78478, saving model to weights-improvement-84-1.7848.hdf5\n",
      "Epoch 85/200\n",
      "266513/266513 [==============================] - 82s 308us/step - loss: 1.7851\n",
      "\n",
      "Epoch 00085: loss did not improve from 1.78478\n",
      "Epoch 86/200\n",
      "266513/266513 [==============================] - 82s 307us/step - loss: 1.7826\n",
      "\n",
      "Epoch 00086: loss improved from 1.78478 to 1.78257, saving model to weights-improvement-86-1.7826.hdf5\n",
      "Epoch 87/200\n",
      "266513/266513 [==============================] - 82s 307us/step - loss: 1.7775\n",
      "\n",
      "Epoch 00087: loss improved from 1.78257 to 1.77752, saving model to weights-improvement-87-1.7775.hdf5\n",
      "Epoch 88/200\n",
      "266513/266513 [==============================] - 82s 306us/step - loss: 1.7749\n",
      "\n",
      "Epoch 00088: loss improved from 1.77752 to 1.77486, saving model to weights-improvement-88-1.7749.hdf5\n",
      "Epoch 89/200\n",
      "266513/266513 [==============================] - 83s 310us/step - loss: 1.7705\n",
      "\n",
      "Epoch 00089: loss improved from 1.77486 to 1.77047, saving model to weights-improvement-89-1.7705.hdf5\n",
      "Epoch 90/200\n",
      "266513/266513 [==============================] - 83s 311us/step - loss: 1.7682\n",
      "\n",
      "Epoch 00090: loss improved from 1.77047 to 1.76822, saving model to weights-improvement-90-1.7682.hdf5\n",
      "Epoch 91/200\n",
      "266513/266513 [==============================] - 82s 307us/step - loss: 1.7657\n",
      "\n",
      "Epoch 00091: loss improved from 1.76822 to 1.76572, saving model to weights-improvement-91-1.7657.hdf5\n",
      "Epoch 92/200\n",
      "266513/266513 [==============================] - 86s 322us/step - loss: 1.7641\n",
      "\n",
      "Epoch 00092: loss improved from 1.76572 to 1.76409, saving model to weights-improvement-92-1.7641.hdf5\n",
      "Epoch 93/200\n",
      "266513/266513 [==============================] - 91s 343us/step - loss: 1.7600\n",
      "\n",
      "Epoch 00093: loss improved from 1.76409 to 1.75999, saving model to weights-improvement-93-1.7600.hdf5\n",
      "Epoch 94/200\n",
      "266513/266513 [==============================] - 96s 360us/step - loss: 1.7549\n",
      "\n",
      "Epoch 00094: loss improved from 1.75999 to 1.75489, saving model to weights-improvement-94-1.7549.hdf5\n",
      "Epoch 95/200\n",
      "266513/266513 [==============================] - 92s 344us/step - loss: 1.7532\n",
      "\n",
      "Epoch 00095: loss improved from 1.75489 to 1.75319, saving model to weights-improvement-95-1.7532.hdf5\n",
      "Epoch 96/200\n",
      "266513/266513 [==============================] - 96s 359us/step - loss: 1.7505\n",
      "\n",
      "Epoch 00096: loss improved from 1.75319 to 1.75050, saving model to weights-improvement-96-1.7505.hdf5\n",
      "Epoch 97/200\n",
      "266513/266513 [==============================] - 91s 340us/step - loss: 1.7481\n",
      "\n",
      "Epoch 00097: loss improved from 1.75050 to 1.74807, saving model to weights-improvement-97-1.7481.hdf5\n",
      "Epoch 98/200\n",
      "266513/266513 [==============================] - 93s 348us/step - loss: 1.7434\n",
      "\n",
      "Epoch 00098: loss improved from 1.74807 to 1.74337, saving model to weights-improvement-98-1.7434.hdf5\n",
      "Epoch 99/200\n",
      "266513/266513 [==============================] - 97s 365us/step - loss: 1.74390s - los\n",
      "\n",
      "Epoch 00099: loss did not improve from 1.74337\n",
      "Epoch 100/200\n",
      "266513/266513 [==============================] - 102s 384us/step - loss: 1.7362s - loss: 1\n",
      "\n",
      "Epoch 00100: loss improved from 1.74337 to 1.73622, saving model to weights-improvement-100-1.7362.hdf5\n",
      "Epoch 101/200\n",
      "266513/266513 [==============================] - 95s 356us/step - loss: 1.7387\n",
      "\n",
      "Epoch 00101: loss did not improve from 1.73622\n",
      "Epoch 102/200\n",
      "266513/266513 [==============================] - 94s 354us/step - loss: 1.7336\n",
      "\n",
      "Epoch 00102: loss improved from 1.73622 to 1.73361, saving model to weights-improvement-102-1.7336.hdf5\n",
      "Epoch 103/200\n",
      "266513/266513 [==============================] - 98s 366us/step - loss: 1.7315\n",
      "\n",
      "Epoch 00103: loss improved from 1.73361 to 1.73147, saving model to weights-improvement-103-1.7315.hdf5\n",
      "Epoch 104/200\n",
      "266513/266513 [==============================] - 91s 341us/step - loss: 1.7268\n",
      "\n",
      "Epoch 00104: loss improved from 1.73147 to 1.72677, saving model to weights-improvement-104-1.7268.hdf5\n",
      "Epoch 105/200\n",
      "266513/266513 [==============================] - 86s 321us/step - loss: 1.7257\n",
      "\n",
      "Epoch 00105: loss improved from 1.72677 to 1.72568, saving model to weights-improvement-105-1.7257.hdf5\n",
      "Epoch 106/200\n",
      "266513/266513 [==============================] - 85s 319us/step - loss: 1.7204\n",
      "\n",
      "Epoch 00106: loss improved from 1.72568 to 1.72036, saving model to weights-improvement-106-1.7204.hdf5\n",
      "Epoch 107/200\n",
      "266513/266513 [==============================] - 88s 329us/step - loss: 1.7224\n",
      "\n",
      "Epoch 00107: loss did not improve from 1.72036\n",
      "Epoch 108/200\n",
      "266513/266513 [==============================] - 89s 332us/step - loss: 1.7177\n",
      "\n",
      "Epoch 00108: loss improved from 1.72036 to 1.71770, saving model to weights-improvement-108-1.7177.hdf5\n",
      "Epoch 109/200\n",
      "266513/266513 [==============================] - 86s 324us/step - loss: 1.7116\n",
      "\n",
      "Epoch 00109: loss improved from 1.71770 to 1.71164, saving model to weights-improvement-109-1.7116.hdf5\n",
      "Epoch 110/200\n",
      "266513/266513 [==============================] - 85s 318us/step - loss: 1.7143\n",
      "\n",
      "Epoch 00110: loss did not improve from 1.71164\n",
      "Epoch 111/200\n",
      "266513/266513 [==============================] - 86s 321us/step - loss: 1.7105\n",
      "\n",
      "Epoch 00111: loss improved from 1.71164 to 1.71048, saving model to weights-improvement-111-1.7105.hdf5\n",
      "Epoch 112/200\n",
      "266513/266513 [==============================] - 85s 320us/step - loss: 1.7086\n",
      "\n",
      "Epoch 00112: loss improved from 1.71048 to 1.70864, saving model to weights-improvement-112-1.7086.hdf5\n",
      "Epoch 113/200\n",
      "266513/266513 [==============================] - 85s 318us/step - loss: 1.7049\n",
      "\n",
      "Epoch 00113: loss improved from 1.70864 to 1.70495, saving model to weights-improvement-113-1.7049.hdf5\n",
      "Epoch 114/200\n",
      "266513/266513 [==============================] - 85s 320us/step - loss: 1.7020\n",
      "\n",
      "Epoch 00114: loss improved from 1.70495 to 1.70204, saving model to weights-improvement-114-1.7020.hdf5\n",
      "Epoch 115/200\n",
      "266513/266513 [==============================] - 86s 322us/step - loss: 1.7012\n",
      "\n",
      "Epoch 00115: loss improved from 1.70204 to 1.70125, saving model to weights-improvement-115-1.7012.hdf5\n",
      "Epoch 116/200\n",
      "266513/266513 [==============================] - 91s 340us/step - loss: 1.6981\n",
      "\n",
      "Epoch 00116: loss improved from 1.70125 to 1.69809, saving model to weights-improvement-116-1.6981.hdf5\n",
      "Epoch 117/200\n",
      "266513/266513 [==============================] - 90s 337us/step - loss: 1.6987\n",
      "\n",
      "Epoch 00117: loss did not improve from 1.69809\n",
      "Epoch 118/200\n",
      "266513/266513 [==============================] - 91s 340us/step - loss: 1.6930\n",
      "\n",
      "Epoch 00118: loss improved from 1.69809 to 1.69303, saving model to weights-improvement-118-1.6930.hdf5\n",
      "Epoch 119/200\n",
      "266513/266513 [==============================] - 91s 341us/step - loss: 1.6907\n",
      "\n",
      "Epoch 00119: loss improved from 1.69303 to 1.69074, saving model to weights-improvement-119-1.6907.hdf5\n",
      "Epoch 120/200\n",
      "266513/266513 [==============================] - 91s 341us/step - loss: 1.6883\n",
      "\n",
      "Epoch 00120: loss improved from 1.69074 to 1.68832, saving model to weights-improvement-120-1.6883.hdf5\n",
      "Epoch 121/200\n",
      "266513/266513 [==============================] - 92s 344us/step - loss: 1.6875\n",
      "\n",
      "Epoch 00121: loss improved from 1.68832 to 1.68754, saving model to weights-improvement-121-1.6875.hdf5\n",
      "Epoch 122/200\n",
      "266513/266513 [==============================] - 91s 341us/step - loss: 1.6849\n",
      "\n",
      "Epoch 00122: loss improved from 1.68754 to 1.68490, saving model to weights-improvement-122-1.6849.hdf5\n",
      "Epoch 123/200\n",
      "266513/266513 [==============================] - 91s 343us/step - loss: 1.6823\n",
      "\n",
      "Epoch 00123: loss improved from 1.68490 to 1.68235, saving model to weights-improvement-123-1.6823.hdf5\n",
      "Epoch 124/200\n",
      "266513/266513 [==============================] - 90s 338us/step - loss: 1.6822\n",
      "\n",
      "Epoch 00124: loss improved from 1.68235 to 1.68217, saving model to weights-improvement-124-1.6822.hdf5\n",
      "Epoch 125/200\n",
      "266513/266513 [==============================] - 90s 337us/step - loss: 1.6810\n",
      "\n",
      "Epoch 00125: loss improved from 1.68217 to 1.68098, saving model to weights-improvement-125-1.6810.hdf5\n",
      "Epoch 126/200\n",
      "266513/266513 [==============================] - 91s 342us/step - loss: 1.6766\n",
      "\n",
      "Epoch 00126: loss improved from 1.68098 to 1.67656, saving model to weights-improvement-126-1.6766.hdf5\n",
      "Epoch 127/200\n",
      "266513/266513 [==============================] - 90s 336us/step - loss: 1.6784\n",
      "\n",
      "Epoch 00127: loss did not improve from 1.67656\n",
      "Epoch 128/200\n",
      "266513/266513 [==============================] - 90s 338us/step - loss: 1.6738\n",
      "\n",
      "Epoch 00128: loss improved from 1.67656 to 1.67383, saving model to weights-improvement-128-1.6738.hdf5\n",
      "Epoch 129/200\n",
      "266513/266513 [==============================] - 90s 338us/step - loss: 1.6690\n",
      "\n",
      "Epoch 00129: loss improved from 1.67383 to 1.66903, saving model to weights-improvement-129-1.6690.hdf5\n",
      "Epoch 130/200\n",
      "266513/266513 [==============================] - 90s 337us/step - loss: 1.6712\n",
      "\n",
      "Epoch 00130: loss did not improve from 1.66903\n",
      "Epoch 131/200\n",
      "266513/266513 [==============================] - 90s 336us/step - loss: 1.6669\n",
      "\n",
      "Epoch 00131: loss improved from 1.66903 to 1.66690, saving model to weights-improvement-131-1.6669.hdf5\n",
      "Epoch 132/200\n",
      "266513/266513 [==============================] - 90s 338us/step - loss: 1.6668\n",
      "\n",
      "Epoch 00132: loss improved from 1.66690 to 1.66675, saving model to weights-improvement-132-1.6668.hdf5\n",
      "Epoch 133/200\n",
      "266513/266513 [==============================] - 90s 337us/step - loss: 1.6639\n",
      "\n",
      "Epoch 00133: loss improved from 1.66675 to 1.66395, saving model to weights-improvement-133-1.6639.hdf5\n",
      "Epoch 134/200\n",
      "266513/266513 [==============================] - 90s 337us/step - loss: 1.6612\n",
      "\n",
      "Epoch 00134: loss improved from 1.66395 to 1.66118, saving model to weights-improvement-134-1.6612.hdf5\n",
      "Epoch 135/200\n",
      "266513/266513 [==============================] - 90s 339us/step - loss: 1.6589\n",
      "\n",
      "Epoch 00135: loss improved from 1.66118 to 1.65892, saving model to weights-improvement-135-1.6589.hdf5\n",
      "Epoch 136/200\n",
      "266513/266513 [==============================] - 90s 337us/step - loss: 1.6595\n",
      "\n",
      "Epoch 00136: loss did not improve from 1.65892\n",
      "Epoch 137/200\n",
      "266513/266513 [==============================] - 90s 339us/step - loss: 1.6577\n",
      "\n",
      "Epoch 00137: loss improved from 1.65892 to 1.65768, saving model to weights-improvement-137-1.6577.hdf5\n",
      "Epoch 138/200\n",
      "266513/266513 [==============================] - 90s 338us/step - loss: 1.6550\n",
      "\n",
      "Epoch 00138: loss improved from 1.65768 to 1.65501, saving model to weights-improvement-138-1.6550.hdf5\n",
      "Epoch 139/200\n",
      "266513/266513 [==============================] - 90s 338us/step - loss: 1.6558\n",
      "\n",
      "Epoch 00139: loss did not improve from 1.65501\n",
      "Epoch 140/200\n",
      "266513/266513 [==============================] - 91s 340us/step - loss: 1.6511\n",
      "\n",
      "Epoch 00140: loss improved from 1.65501 to 1.65108, saving model to weights-improvement-140-1.6511.hdf5\n",
      "Epoch 141/200\n",
      "266513/266513 [==============================] - 90s 337us/step - loss: 1.6515\n",
      "\n",
      "Epoch 00141: loss did not improve from 1.65108\n",
      "Epoch 142/200\n",
      "266513/266513 [==============================] - 90s 338us/step - loss: 1.6486\n",
      "\n",
      "Epoch 00142: loss improved from 1.65108 to 1.64860, saving model to weights-improvement-142-1.6486.hdf5\n",
      "Epoch 143/200\n",
      "266513/266513 [==============================] - 90s 339us/step - loss: 1.6456\n",
      "\n",
      "Epoch 00143: loss improved from 1.64860 to 1.64561, saving model to weights-improvement-143-1.6456.hdf5\n",
      "Epoch 144/200\n",
      "266513/266513 [==============================] - 90s 336us/step - loss: 1.6443\n",
      "\n",
      "Epoch 00144: loss improved from 1.64561 to 1.64430, saving model to weights-improvement-144-1.6443.hdf5\n",
      "Epoch 145/200\n",
      "266513/266513 [==============================] - 90s 339us/step - loss: 1.6435\n",
      "\n",
      "Epoch 00145: loss improved from 1.64430 to 1.64347, saving model to weights-improvement-145-1.6435.hdf5\n",
      "Epoch 146/200\n",
      "266513/266513 [==============================] - 90s 339us/step - loss: 1.64291s -\n",
      "\n",
      "Epoch 00146: loss improved from 1.64347 to 1.64293, saving model to weights-improvement-146-1.6429.hdf5\n",
      "Epoch 147/200\n",
      "266513/266513 [==============================] - 90s 338us/step - loss: 1.6419\n",
      "\n",
      "Epoch 00147: loss improved from 1.64293 to 1.64192, saving model to weights-improvement-147-1.6419.hdf5\n",
      "Epoch 148/200\n",
      "266513/266513 [==============================] - 90s 338us/step - loss: 1.6377\n",
      "\n",
      "Epoch 00148: loss improved from 1.64192 to 1.63775, saving model to weights-improvement-148-1.6377.hdf5\n",
      "Epoch 149/200\n",
      "266513/266513 [==============================] - 91s 341us/step - loss: 1.6363\n",
      "\n",
      "Epoch 00149: loss improved from 1.63775 to 1.63627, saving model to weights-improvement-149-1.6363.hdf5\n",
      "Epoch 150/200\n",
      "266513/266513 [==============================] - 90s 339us/step - loss: 1.6336\n",
      "\n",
      "Epoch 00150: loss improved from 1.63627 to 1.63356, saving model to weights-improvement-150-1.6336.hdf5\n",
      "Epoch 151/200\n",
      "266513/266513 [==============================] - 90s 337us/step - loss: 1.6329\n",
      "\n",
      "Epoch 00151: loss improved from 1.63356 to 1.63292, saving model to weights-improvement-151-1.6329.hdf5\n",
      "Epoch 152/200\n",
      "266513/266513 [==============================] - 87s 328us/step - loss: 1.6316\n",
      "\n",
      "Epoch 00152: loss improved from 1.63292 to 1.63161, saving model to weights-improvement-152-1.6316.hdf5\n",
      "Epoch 153/200\n",
      "266513/266513 [==============================] - 83s 313us/step - loss: 1.6317\n",
      "\n",
      "Epoch 00153: loss did not improve from 1.63161\n",
      "Epoch 154/200\n",
      "266513/266513 [==============================] - 83s 312us/step - loss: 1.6307\n",
      "\n",
      "Epoch 00154: loss improved from 1.63161 to 1.63073, saving model to weights-improvement-154-1.6307.hdf5\n",
      "Epoch 155/200\n",
      "266513/266513 [==============================] - 84s 315us/step - loss: 1.6291\n",
      "\n",
      "Epoch 00155: loss improved from 1.63073 to 1.62912, saving model to weights-improvement-155-1.6291.hdf5\n",
      "Epoch 156/200\n",
      "266513/266513 [==============================] - 83s 313us/step - loss: 1.6246\n",
      "\n",
      "Epoch 00156: loss improved from 1.62912 to 1.62464, saving model to weights-improvement-156-1.6246.hdf5\n",
      "Epoch 157/200\n",
      "266513/266513 [==============================] - 84s 315us/step - loss: 1.6233\n",
      "\n",
      "Epoch 00157: loss improved from 1.62464 to 1.62326, saving model to weights-improvement-157-1.6233.hdf5\n",
      "Epoch 158/200\n",
      "266513/266513 [==============================] - 83s 312us/step - loss: 1.6215\n",
      "\n",
      "Epoch 00158: loss improved from 1.62326 to 1.62154, saving model to weights-improvement-158-1.6215.hdf5\n",
      "Epoch 159/200\n",
      "266513/266513 [==============================] - 83s 313us/step - loss: 1.6207\n",
      "\n",
      "Epoch 00159: loss improved from 1.62154 to 1.62070, saving model to weights-improvement-159-1.6207.hdf5\n",
      "Epoch 160/200\n",
      "266513/266513 [==============================] - 84s 313us/step - loss: 1.6207\n",
      "\n",
      "Epoch 00160: loss improved from 1.62070 to 1.62068, saving model to weights-improvement-160-1.6207.hdf5\n",
      "Epoch 161/200\n",
      "266513/266513 [==============================] - 84s 316us/step - loss: 1.6182\n",
      "\n",
      "Epoch 00161: loss improved from 1.62068 to 1.61825, saving model to weights-improvement-161-1.6182.hdf5\n",
      "Epoch 162/200\n",
      "266513/266513 [==============================] - 84s 315us/step - loss: 1.6205\n",
      "\n",
      "Epoch 00162: loss did not improve from 1.61825\n",
      "Epoch 163/200\n",
      "266513/266513 [==============================] - 88s 332us/step - loss: 1.6163\n",
      "\n",
      "Epoch 00163: loss improved from 1.61825 to 1.61633, saving model to weights-improvement-163-1.6163.hdf5\n",
      "Epoch 164/200\n",
      "266513/266513 [==============================] - 91s 342us/step - loss: 1.6127\n",
      "\n",
      "Epoch 00164: loss improved from 1.61633 to 1.61274, saving model to weights-improvement-164-1.6127.hdf5\n",
      "Epoch 165/200\n",
      "266513/266513 [==============================] - 93s 350us/step - loss: 1.6126\n",
      "\n",
      "Epoch 00165: loss improved from 1.61274 to 1.61262, saving model to weights-improvement-165-1.6126.hdf5\n",
      "Epoch 166/200\n",
      "266513/266513 [==============================] - 91s 341us/step - loss: 1.6113\n",
      "\n",
      "Epoch 00166: loss improved from 1.61262 to 1.61131, saving model to weights-improvement-166-1.6113.hdf5\n",
      "Epoch 167/200\n",
      "266513/266513 [==============================] - 90s 338us/step - loss: 1.6086\n",
      "\n",
      "Epoch 00167: loss improved from 1.61131 to 1.60855, saving model to weights-improvement-167-1.6086.hdf5\n",
      "Epoch 168/200\n",
      "266513/266513 [==============================] - 90s 337us/step - loss: 1.6104\n",
      "\n",
      "Epoch 00168: loss did not improve from 1.60855\n",
      "Epoch 169/200\n",
      "266513/266513 [==============================] - 90s 337us/step - loss: 1.6085\n",
      "\n",
      "Epoch 00169: loss improved from 1.60855 to 1.60852, saving model to weights-improvement-169-1.6085.hdf5\n",
      "Epoch 170/200\n",
      "266513/266513 [==============================] - 91s 340us/step - loss: 1.6054\n",
      "\n",
      "Epoch 00170: loss improved from 1.60852 to 1.60538, saving model to weights-improvement-170-1.6054.hdf5\n",
      "Epoch 171/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "266513/266513 [==============================] - 89s 335us/step - loss: 1.6067\n",
      "\n",
      "Epoch 00171: loss did not improve from 1.60538\n",
      "Epoch 172/200\n",
      "266513/266513 [==============================] - 89s 336us/step - loss: 1.6032\n",
      "\n",
      "Epoch 00172: loss improved from 1.60538 to 1.60316, saving model to weights-improvement-172-1.6032.hdf5\n",
      "Epoch 173/200\n",
      "266513/266513 [==============================] - 90s 338us/step - loss: 1.6046\n",
      "\n",
      "Epoch 00173: loss did not improve from 1.60316\n",
      "Epoch 174/200\n",
      "266513/266513 [==============================] - 89s 335us/step - loss: 1.6005\n",
      "\n",
      "Epoch 00174: loss improved from 1.60316 to 1.60053, saving model to weights-improvement-174-1.6005.hdf5\n",
      "Epoch 175/200\n",
      "266513/266513 [==============================] - 89s 335us/step - loss: 1.5997\n",
      "\n",
      "Epoch 00175: loss improved from 1.60053 to 1.59974, saving model to weights-improvement-175-1.5997.hdf5\n",
      "Epoch 176/200\n",
      "266513/266513 [==============================] - 90s 339us/step - loss: 1.59911s \n",
      "\n",
      "Epoch 00176: loss improved from 1.59974 to 1.59908, saving model to weights-improvement-176-1.5991.hdf5\n",
      "Epoch 177/200\n",
      "266513/266513 [==============================] - 90s 337us/step - loss: 1.5972\n",
      "\n",
      "Epoch 00177: loss improved from 1.59908 to 1.59719, saving model to weights-improvement-177-1.5972.hdf5\n",
      "Epoch 178/200\n",
      "266513/266513 [==============================] - 89s 336us/step - loss: 1.5966\n",
      "\n",
      "Epoch 00178: loss improved from 1.59719 to 1.59655, saving model to weights-improvement-178-1.5966.hdf5\n",
      "Epoch 179/200\n",
      "266513/266513 [==============================] - 89s 335us/step - loss: 1.5952\n",
      "\n",
      "Epoch 00179: loss improved from 1.59655 to 1.59521, saving model to weights-improvement-179-1.5952.hdf5\n",
      "Epoch 180/200\n",
      "266513/266513 [==============================] - 90s 338us/step - loss: 1.5954\n",
      "\n",
      "Epoch 00180: loss did not improve from 1.59521\n",
      "Epoch 181/200\n",
      "266513/266513 [==============================] - 90s 337us/step - loss: 1.59061s \n",
      "\n",
      "Epoch 00181: loss improved from 1.59521 to 1.59062, saving model to weights-improvement-181-1.5906.hdf5\n",
      "Epoch 182/200\n",
      "266513/266513 [==============================] - 89s 336us/step - loss: 1.5917\n",
      "\n",
      "Epoch 00182: loss did not improve from 1.59062\n",
      "Epoch 183/200\n",
      "266513/266513 [==============================] - 90s 338us/step - loss: 1.5903\n",
      "\n",
      "Epoch 00183: loss improved from 1.59062 to 1.59033, saving model to weights-improvement-183-1.5903.hdf5\n",
      "Epoch 184/200\n",
      "266513/266513 [==============================] - 89s 335us/step - loss: 1.5887\n",
      "\n",
      "Epoch 00184: loss improved from 1.59033 to 1.58869, saving model to weights-improvement-184-1.5887.hdf5\n",
      "Epoch 185/200\n",
      "266513/266513 [==============================] - 90s 336us/step - loss: 1.5875\n",
      "\n",
      "Epoch 00185: loss improved from 1.58869 to 1.58746, saving model to weights-improvement-185-1.5875.hdf5\n",
      "Epoch 186/200\n",
      "266513/266513 [==============================] - 90s 337us/step - loss: 1.5855\n",
      "\n",
      "Epoch 00186: loss improved from 1.58746 to 1.58547, saving model to weights-improvement-186-1.5855.hdf5\n",
      "Epoch 187/200\n",
      "266513/266513 [==============================] - 90s 336us/step - loss: 1.5858\n",
      "\n",
      "Epoch 00187: loss did not improve from 1.58547\n",
      "Epoch 188/200\n",
      "266513/266513 [==============================] - 89s 335us/step - loss: 1.5833\n",
      "\n",
      "Epoch 00188: loss improved from 1.58547 to 1.58328, saving model to weights-improvement-188-1.5833.hdf5\n",
      "Epoch 189/200\n",
      "266513/266513 [==============================] - 89s 335us/step - loss: 1.58370s - loss: 1.58\n",
      "\n",
      "Epoch 00189: loss did not improve from 1.58328\n",
      "Epoch 190/200\n",
      "266513/266513 [==============================] - 90s 338us/step - loss: 1.5824\n",
      "\n",
      "Epoch 00190: loss improved from 1.58328 to 1.58239, saving model to weights-improvement-190-1.5824.hdf5\n",
      "Epoch 191/200\n",
      "266513/266513 [==============================] - 90s 336us/step - loss: 1.5813\n",
      "\n",
      "Epoch 00191: loss improved from 1.58239 to 1.58125, saving model to weights-improvement-191-1.5813.hdf5\n",
      "Epoch 192/200\n",
      "266513/266513 [==============================] - 90s 336us/step - loss: 1.5773\n",
      "\n",
      "Epoch 00192: loss improved from 1.58125 to 1.57731, saving model to weights-improvement-192-1.5773.hdf5\n",
      "Epoch 193/200\n",
      "266513/266513 [==============================] - 90s 336us/step - loss: 1.5800\n",
      "\n",
      "Epoch 00193: loss did not improve from 1.57731\n",
      "Epoch 194/200\n",
      "266513/266513 [==============================] - 89s 335us/step - loss: 1.57870s - loss: 1.5\n",
      "\n",
      "Epoch 00194: loss did not improve from 1.57731\n",
      "Epoch 195/200\n",
      "266513/266513 [==============================] - 90s 336us/step - loss: 1.5774\n",
      "\n",
      "Epoch 00195: loss did not improve from 1.57731\n",
      "Epoch 196/200\n",
      "266513/266513 [==============================] - 90s 337us/step - loss: 1.5782\n",
      "\n",
      "Epoch 00196: loss did not improve from 1.57731\n",
      "Epoch 197/200\n",
      "266513/266513 [==============================] - 90s 337us/step - loss: 1.5759\n",
      "\n",
      "Epoch 00197: loss improved from 1.57731 to 1.57595, saving model to weights-improvement-197-1.5759.hdf5\n",
      "Epoch 198/200\n",
      "266513/266513 [==============================] - 90s 336us/step - loss: 1.5733\n",
      "\n",
      "Epoch 00198: loss improved from 1.57595 to 1.57334, saving model to weights-improvement-198-1.5733.hdf5\n",
      "Epoch 199/200\n",
      "266513/266513 [==============================] - 89s 335us/step - loss: 1.5738\n",
      "\n",
      "Epoch 00199: loss did not improve from 1.57334\n",
      "Epoch 200/200\n",
      "266513/266513 [==============================] - 90s 337us/step - loss: 1.5734\n",
      "\n",
      "Epoch 00200: loss did not improve from 1.57334\n",
      "Seed #1:\n",
      "\" w looked a \"\n",
      "Seed #2:\n",
      "\"  two glass \"\n",
      "Seed #3:\n",
      "\" don't worr \"\n",
      "Seed #4:\n",
      "\"  himself k \"\n",
      "Seed #5:\n",
      "\" nd thought \"\n",
      "Seed #6:\n",
      "\" t the plan \"\n",
      "Seed #7:\n",
      "\" e in the t \"\n",
      "Seed #8:\n",
      "\" e dust par \"\n",
      "Seed #9:\n",
      "\" s it exact \"\n",
      "Seed #10:\n",
      "\"  ”Now Eart \"\n",
      "Seed #11:\n",
      "\" of gathere \"\n",
      "Seed #12:\n",
      "\" use Damogr \"\n",
      "Seed #13:\n",
      "\" hat happen \"\n",
      "Seed #14:\n",
      "\"  of light  \"\n",
      "Seed #15:\n",
      "\" pened on t \"\n",
      "Seed #16:\n",
      "\" touched. ” \"\n",
      "Seed #17:\n",
      "\" d in the m \"\n",
      "Seed #18:\n",
      "\" on them. T \"\n",
      "Seed #19:\n",
      "\"  were rake \"\n",
      "Seed #20:\n",
      "\"  you doing \"\n",
      "Seed #21:\n",
      "\"  to that,  \"\n",
      "Seed #22:\n",
      "\"  ancient p \"\n",
      "Seed #23:\n",
      "\" n and get  \"\n",
      "Seed #24:\n",
      "\" ” mimicked \"\n",
      "Seed #25:\n",
      "\" e stations \"\n",
      "n introenc\n",
      " transport\n",
      "y bbduuins\n",
      "ac boot ar\n",
      " ano slete\n",
      "et Earth, \n",
      "ore and to\n",
      "tenns to t\n",
      "ly the soo\n",
      "hlmng brd \n",
      "d thet thr\n",
      "an the poo\n",
      "ed to the \n",
      "aeout iv t\n",
      "he sooe wo\n",
      "In, woatm'\n",
      "oden ole a\n",
      "he compute\n",
      "e and whel\n",
      " the sooe \n",
      "lnt oefh t\n",
      "iit asound\n",
      "oi wou tol\n",
      " Mrrdlt ai\n",
      " hn whu hi\n"
     ]
    }
   ],
   "source": [
    "hhg_texts = write_weights(hhg_fulltext,10,200,25,verbose=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
